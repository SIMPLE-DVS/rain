{
    "nodes": [
        {
            "clazz": "TrainTestSampleTargetSplit",
            "package": "rain.nodes.sklearn.functions.TrainTestSampleTargetSplit",
            "input": {
                "sample_dataset": "DataFrame",
                "target_dataset": "DataFrame"
            },
            "output": {
                "sample_train_dataset": "DataFrame",
                "sample_test_dataset": "DataFrame",
                "target_train_dataset": "DataFrame",
                "target_test_dataset": "DataFrame"
            },
            "parameter": [
                {
                    "name": "test_size",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": null,
                    "description": null
                },
                {
                    "name": "train_size",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": null,
                    "description": null
                },
                {
                    "name": "random_state",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": null,
                    "description": null
                },
                {
                    "name": "shuffle",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": true,
                    "description": null
                }
            ],
            "methods": [],
            "tags": {
                "library": "Sklearn",
                "type": "Transformer"
            },
            "name": "Train Test Sample Target Split",
            "description": ""
        },
        {
            "clazz": "SparkPipelineNode",
            "package": "rain.nodes.spark.pipeline.spark_pipeline.SparkPipelineNode",
            "input": {
                "dataset": "DataFrame"
            },
            "output": {
                "model": "PipelineModel"
            },
            "parameter": [
                {
                    "name": "stages",
                    "type": null,
                    "is_mandatory": true,
                    "default_value": null,
                    "description": null
                }
            ],
            "methods": null,
            "tags": {
                "library": "PySpark",
                "type": "Estimator"
            },
            "name": "Spark Pipeline Node",
            "description": "Represent a Spark Pipeline consisting of SparkNode (stages)"
        },
        {
            "clazz": "MongoCSVWriter",
            "package": "rain.nodes.mongodb.database_io.MongoCSVWriter",
            "input": {
                "dataset": "DataFrame"
            },
            "output": null,
            "parameter": [
                {
                    "name": "connection",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Hostname or IP address or Unix domain socket path of a single MongoDB instance to connect to, or a mongodb URI"
                },
                {
                    "name": "db",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Name of the database to connect to."
                },
                {
                    "name": "coll",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Name of the collection to connect to."
                }
            ],
            "methods": null,
            "tags": {
                "library": "PyMongo",
                "type": "Output"
            },
            "name": "Mongo CSV Writer",
            "description": "Write a Pandas Dataframe into a MongoDB collection."
        },
        {
            "clazz": "PandasRenameColumn",
            "package": "rain.nodes.pandas.transform_nodes.PandasRenameColumn",
            "input": {
                "dataset": "DataFrame"
            },
            "output": {
                "dataset": "DataFrame"
            },
            "parameter": [
                {
                    "name": "columns",
                    "type": "list[str]",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Column names to assign to the DataFrame. The order is relevant."
                }
            ],
            "methods": null,
            "tags": {
                "library": "Pandas",
                "type": "Transformer"
            },
            "name": "Pandas Rename Column",
            "description": "Sets column names for a pandas DataFrame."
        },
        {
            "clazz": "SparkCSVLoader",
            "package": "rain.nodes.spark.spark_input.SparkCSVLoader",
            "input": null,
            "output": {
                "dataset": "DataFrame"
            },
            "parameter": [
                {
                    "name": "path",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Path of the csv file."
                },
                {
                    "name": "header",
                    "type": "bool",
                    "is_mandatory": false,
                    "default_value": false,
                    "description": "Uses the first line as names of columns."
                },
                {
                    "name": "schema",
                    "type": "bool",
                    "is_mandatory": false,
                    "default_value": false,
                    "description": "Infers the input schema automatically from data. It requires one extra pass over the data."
                }
            ],
            "methods": null,
            "tags": {
                "library": "PySpark",
                "type": "Input"
            },
            "name": "Spark CSV Loader",
            "description": "Loads a CSV file as a Spark DataFrame."
        },
        {
            "clazz": "IrisDatasetLoader",
            "package": "rain.nodes.sklearn.loaders.IrisDatasetLoader",
            "input": null,
            "output": {
                "dataset": "DataFrame",
                "target": "DataFrame"
            },
            "parameter": [
                {
                    "name": "separate_target",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": false,
                    "description": null
                }
            ],
            "methods": null,
            "tags": {
                "library": "Sklearn",
                "type": "Input"
            },
            "name": "Iris Dataset Loader",
            "description": "Loads the iris dataset as a pandas DataFrame."
        },
        {
            "clazz": "SparkSplitDataset",
            "package": "rain.nodes.spark.data_wrangling.SparkSplitDataset",
            "input": {
                "dataset": "DataFrame"
            },
            "output": {
                "dataset": "DataFrame",
                "train_dataset": "DataFrame",
                "test_dataset": "DataFrame"
            },
            "parameter": [
                {
                    "name": "train",
                    "type": "float",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Percentage of the dataset to split into a train dataset."
                },
                {
                    "name": "test",
                    "type": "float",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Percentage of the dataset to split into a test dataset."
                }
            ],
            "methods": null,
            "tags": {
                "library": "PySpark",
                "type": "Transformer"
            },
            "name": "Spark Split Dataset",
            "description": "Splits a Spark DataFrame in two DataFrames, train and test."
        },
        {
            "clazz": "CustomNode",
            "package": "rain.nodes.custom.custom.CustomNode",
            "input": {},
            "output": {},
            "parameter": [
                {
                    "name": "use_function",
                    "type": null,
                    "is_mandatory": true,
                    "default_value": null,
                    "description": null
                },
                {
                    "name": "kwargs",
                    "type": null,
                    "is_mandatory": true,
                    "default_value": null,
                    "description": null
                }
            ],
            "methods": null,
            "tags": {
                "library": "Base",
                "type": "Custom"
            },
            "name": "Custom Node",
            "description": "A node that can contain user-defined Python code."
        },
        {
            "clazz": "PandasReplaceColumn",
            "package": "rain.nodes.pandas.transform_nodes.PandasReplaceColumn",
            "input": {
                "column": "Series"
            },
            "output": {
                "column": "Series"
            },
            "parameter": [
                {
                    "name": "first_value",
                    "type": "Any",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Value used when the condition is True."
                },
                {
                    "name": "second_value",
                    "type": "Any",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Value used when the condition is True."
                }
            ],
            "methods": null,
            "tags": {
                "library": "Pandas",
                "type": "Transformer"
            },
            "name": "Pandas Replace Column",
            "description": "Node used to replace the boolean values of a Pandas Series with other values given by the user."
        },
        {
            "clazz": "PandasPivot",
            "package": "rain.nodes.pandas.transform_nodes.PandasPivot",
            "input": {
                "dataset": "DataFrame"
            },
            "output": {
                "dataset": "DataFrame"
            },
            "parameter": [
                {
                    "name": "rows",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Name of the column whose values will be the rows of the pivot."
                },
                {
                    "name": "columns",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Name of the column whose values will be the columns of the pivot."
                },
                {
                    "name": "values",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Name of the column whose values will be the values of the pivot."
                },
                {
                    "name": "aggfunc",
                    "type": "str",
                    "is_mandatory": false,
                    "default_value": "mean",
                    "description": "Function to use for the aggregation."
                },
                {
                    "name": "fill_value",
                    "type": "int",
                    "is_mandatory": false,
                    "default_value": 0,
                    "description": "Value to replace missing values with."
                },
                {
                    "name": "dropna",
                    "type": "bool",
                    "is_mandatory": false,
                    "default_value": true,
                    "description": "Do not include columns whose entries are all NaN."
                },
                {
                    "name": "sort",
                    "type": "bool",
                    "is_mandatory": false,
                    "default_value": true,
                    "description": "Specifies if the result should be sorted."
                }
            ],
            "methods": null,
            "tags": {
                "library": "Pandas",
                "type": "Transformer"
            },
            "name": "Pandas Pivot",
            "description": "Transforms a DataFrame into a Pivot from the given rows, columns and values."
        },
        {
            "clazz": "PandasDropNan",
            "package": "rain.nodes.pandas.transform_nodes.PandasDropNan",
            "input": {
                "dataset": "DataFrame"
            },
            "output": {
                "dataset": "DataFrame"
            },
            "parameter": [
                {
                    "name": "axis",
                    "type": "{'rows', 'columns'}",
                    "is_mandatory": false,
                    "default_value": "rows",
                    "description": "The axis from where to remove the nan values."
                },
                {
                    "name": "how",
                    "type": "{'any', 'all'}",
                    "is_mandatory": false,
                    "default_value": "any",
                    "description": "Whether to remove a row or a column which either contains any nan value or contains all nan values."
                }
            ],
            "methods": null,
            "tags": {
                "library": "Pandas",
                "type": "Transformer"
            },
            "name": "Pandas Drop Nan",
            "description": "Drops rows or columns that either only contains a nan or that has all nan values."
        },
        {
            "clazz": "SparkModelLoader",
            "package": "rain.nodes.spark.spark_input.SparkModelLoader",
            "input": null,
            "output": {
                "model": "PipelineModel"
            },
            "parameter": [
                {
                    "name": "path",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Path of the csv file."
                }
            ],
            "methods": null,
            "tags": {
                "library": "PySpark",
                "type": "Input"
            },
            "name": "Spark Model Loader",
            "description": "Loads a file as a Spark Model."
        },
        {
            "clazz": "PandasFilterRows",
            "package": "rain.nodes.pandas.transform_nodes.PandasFilterRows",
            "input": {
                "dataset": "DataFrame",
                "selected_rows": "Series"
            },
            "output": {
                "dataset": "DataFrame"
            },
            "parameter": [],
            "methods": null,
            "tags": {
                "library": "Pandas",
                "type": "Transformer"
            },
            "name": "Pandas Filter Rows",
            "description": "PandasFilterRows manages filtering of rows that have been previously selected."
        },
        {
            "clazz": "MongoCSVReader",
            "package": "rain.nodes.mongodb.database_io.MongoCSVReader",
            "input": null,
            "output": {
                "dataset": "DataFrame"
            },
            "parameter": [
                {
                    "name": "connection",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Hostname or IP address or Unix domain socket path of a single MongoDB instance to connect to, or a mongodb URI"
                },
                {
                    "name": "db",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Name of the database to connect to."
                },
                {
                    "name": "coll",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Name of the collection to connect to."
                },
                {
                    "name": "filter",
                    "type": "dict",
                    "is_mandatory": false,
                    "default_value": null,
                    "description": "A SON object specifying elements which must be present for a document to be included in the result set"
                },
                {
                    "name": "projection",
                    "type": "dict",
                    "is_mandatory": false,
                    "default_value": null,
                    "description": "A dict to exclude fields from the result (e.g. projection={'_id': False})"
                }
            ],
            "methods": null,
            "tags": {
                "library": "PyMongo",
                "type": "Input"
            },
            "name": "Mongo CSV Reader",
            "description": "Read a Pandas Dataframe from a MongoDB collection."
        },
        {
            "clazz": "DaviesBouldinScore",
            "package": "rain.nodes.sklearn.functions.DaviesBouldinScore",
            "input": {
                "samples_dataset": "DataFrame",
                "labels": "DataFrame"
            },
            "output": {
                "score": "float"
            },
            "parameter": [],
            "methods": [],
            "tags": {
                "library": "Sklearn",
                "type": "Metrics"
            },
            "name": "Davies Bouldin Score",
            "description": "Computes the Davies-Bouldin score. The score is defined as the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of within-cluster distances to between-cluster distances. Thus, clusters which are farther apart and less dispersed will result in a better score. The minimum score is zero, with lower values indicating better clustering."
        },
        {
            "clazz": "PandasSelectRows",
            "package": "rain.nodes.pandas.transform_nodes.PandasSelectRows",
            "input": {
                "dataset": "DataFrame"
            },
            "output": {
                "selection": "Series"
            },
            "parameter": [
                {
                    "name": "select_nan",
                    "type": "bool",
                    "is_mandatory": false,
                    "default_value": false,
                    "description": "Select rows with at least one NaN value."
                },
                {
                    "name": "conditions",
                    "type": "List[str]",
                    "is_mandatory": false,
                    "default_value": null,
                    "description": "List of conditions to select rows."
                }
            ],
            "methods": null,
            "tags": {
                "library": "Pandas",
                "type": "Transformer"
            },
            "name": "Pandas Select Rows",
            "description": "PandasSelectRows manages selection of rows, which can later be filtered or deleted."
        },
        {
            "clazz": "SparkSaveDataset",
            "package": "rain.nodes.spark.spark_output.SparkSaveDataset",
            "input": {
                "dataset": "DataFrame"
            },
            "output": null,
            "parameter": [
                {
                    "name": "path",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "String representing the path where to save the dataset"
                },
                {
                    "name": "index",
                    "type": "bool",
                    "is_mandatory": false,
                    "default_value": true,
                    "description": "String representing the path where to save the dataset"
                }
            ],
            "methods": null,
            "tags": {
                "library": "PySpark",
                "type": "Output"
            },
            "name": "Spark Save Dataset",
            "description": "Save a Spark Dataframe in a .csv format"
        },
        {
            "clazz": "SklearnLinearSVC",
            "package": "rain.nodes.sklearn.svm.SklearnLinearSVC",
            "input": {
                "fit_dataset": "DataFrame",
                "predict_dataset": "DataFrame",
                "score_dataset": "DataFrame",
                "fit_targets": "DataFrame",
                "score_targets": "DataFrame"
            },
            "output": {
                "fitted_model": "BaseEstimator",
                "predictions": "DataFrame",
                "score_value": "float"
            },
            "parameter": [
                {
                    "name": "execute",
                    "type": null,
                    "is_mandatory": true,
                    "default_value": null,
                    "description": null
                },
                {
                    "name": "penalty",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": "l2",
                    "description": null
                },
                {
                    "name": "loss",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": "squared_hinge",
                    "description": null
                },
                {
                    "name": "dual",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": true,
                    "description": null
                },
                {
                    "name": "tol",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": 0.0001,
                    "description": null
                },
                {
                    "name": "C",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": 1.0,
                    "description": null
                },
                {
                    "name": "multi_class",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": "ovr",
                    "description": null
                },
                {
                    "name": "fit_intercept",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": true,
                    "description": null
                },
                {
                    "name": "intercept_scaling",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": 1,
                    "description": null
                },
                {
                    "name": "class_weight",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": null,
                    "description": null
                },
                {
                    "name": "verbose",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": 0,
                    "description": null
                },
                {
                    "name": "random_state",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": null,
                    "description": null
                },
                {
                    "name": "max_iter",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": 1000,
                    "description": null
                }
            ],
            "methods": [
                "fit",
                "predict",
                "score"
            ],
            "tags": {
                "library": "Sklearn",
                "type": "Classifier"
            },
            "name": "Sklearn Linear SVC",
            "description": ""
        },
        {
            "clazz": "SparkSaveModel",
            "package": "rain.nodes.spark.spark_output.SparkSaveModel",
            "input": {
                "model": "PipelineModel"
            },
            "output": null,
            "parameter": [
                {
                    "name": "path",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "String representing the path where to save the model"
                }
            ],
            "methods": null,
            "tags": {
                "library": "PySpark",
                "type": "Output"
            },
            "name": "Spark Save Model",
            "description": "Save a trained PipelineModel"
        },
        {
            "clazz": "PandasColumnsFiltering",
            "package": "rain.nodes.pandas.transform_nodes.PandasColumnsFiltering",
            "input": {
                "dataset": "DataFrame"
            },
            "output": {
                "dataset": "DataFrame"
            },
            "parameter": [
                {
                    "name": "column_indexes",
                    "type": "List[int]",
                    "is_mandatory": false,
                    "default_value": null,
                    "description": "Filters the dataset selecting the given indexes. Uses the pandas iloc function."
                },
                {
                    "name": "column_names",
                    "type": "List[str]",
                    "is_mandatory": false,
                    "default_value": null,
                    "description": "Filters the dataset selecting the given column labels. Uses the pandas filter function."
                },
                {
                    "name": "columns_like",
                    "type": "str",
                    "is_mandatory": false,
                    "default_value": null,
                    "description": "Keep columns for which the given string is a substring of the column label."
                },
                {
                    "name": "columns_regex",
                    "type": "str",
                    "is_mandatory": false,
                    "default_value": null,
                    "description": "Keep columns for which column labels match a given pattern."
                },
                {
                    "name": "columns_range",
                    "type": "Tuple[int, int]",
                    "is_mandatory": false,
                    "default_value": null,
                    "description": "Keep columns for which index falls withing the given range (from, to (excluded))."
                },
                {
                    "name": "columns_type",
                    "type": "str or List[str]",
                    "is_mandatory": false,
                    "default_value": null,
                    "description": "Type to assign to columns. It can be either a string, meaning that it will try to apply the chosen type to all the columns, or a list of strings, one for each column, meaning that it will try to assign a chosen type to each column in order."
                }
            ],
            "methods": null,
            "tags": {
                "library": "Pandas",
                "type": "Transformer"
            },
            "name": "Pandas Columns Filtering",
            "description": "PandasColumnsFiltering manages filtering of columns. This node gives access to several functionalities such as: - select columns by their indexes; - select columns by their names (labels); - select columns containing a substring in their names; - select columns that match a regex; - select columns in a range of indexes; - assign a type to a column. Every parameter but 'columns_type' is mutually exclusive, meaning that only one can be used."
        },
        {
            "clazz": "PandasCSVLoader",
            "package": "rain.nodes.pandas.pandas_io.PandasCSVLoader",
            "input": null,
            "output": {
                "dataset": "DataFrame"
            },
            "parameter": [
                {
                    "name": "path",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Of the CSV file."
                },
                {
                    "name": "delim",
                    "type": "str",
                    "is_mandatory": false,
                    "default_value": ",",
                    "description": "Delimiter symbol of the CSV file."
                }
            ],
            "methods": null,
            "tags": {
                "library": "Pandas",
                "type": "Input"
            },
            "name": "Pandas CSV Loader",
            "description": "Loads a pandas DataFrame from a CSV file."
        },
        {
            "clazz": "TrainTestDatasetSplit",
            "package": "rain.nodes.sklearn.functions.TrainTestDatasetSplit",
            "input": {
                "dataset": "DataFrame"
            },
            "output": {
                "train_dataset": "DataFrame",
                "test_dataset": "DataFrame"
            },
            "parameter": [
                {
                    "name": "test_size",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": null,
                    "description": null
                },
                {
                    "name": "train_size",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": null,
                    "description": null
                },
                {
                    "name": "random_state",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": null,
                    "description": null
                },
                {
                    "name": "shuffle",
                    "type": null,
                    "is_mandatory": false,
                    "default_value": true,
                    "description": null
                }
            ],
            "methods": [],
            "tags": {
                "library": "Sklearn",
                "type": "Transformer"
            },
            "name": "Train Test Dataset Split",
            "description": ""
        },
        {
            "clazz": "Tokenizer",
            "package": "rain.nodes.spark.pipeline.stages.Tokenizer",
            "input": {
                "dataset": "DataFrame"
            },
            "output": {
                "dataset": "DataFrame"
            },
            "parameter": [
                {
                    "name": "in_col",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "The name of the input column"
                },
                {
                    "name": "out_col",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "The name of the output column"
                }
            ],
            "methods": null,
            "tags": {
                "library": "PySpark",
                "type": "Transformer"
            },
            "name": "Tokenizer",
            "description": "Represent a Spark Tokenizer used to split text in individual term."
        },
        {
            "clazz": "PandasSequence",
            "package": "rain.nodes.pandas.transform_nodes.PandasSequence",
            "input": {
                "dataset": "DataFrame"
            },
            "output": {
                "dataset": "DataFrame"
            },
            "parameter": [
                {
                    "name": "stages",
                    "type": "list of PandasTransformer",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "ordered in an execution sequence. They must all be PandasNodes, hence have a 'dataset' variable used for input and output."
                }
            ],
            "methods": null,
            "tags": {
                "library": "Pandas",
                "type": "Transformer"
            },
            "name": "Pandas Sequence",
            "description": "PandasSequence wraps a list of nodes that must be executed in sequence into a single node. Intermediate values are passed along the chain using the 'dataset' variable, hence only PandasNodes can be used within a sequence."
        },
        {
            "clazz": "LogisticRegression",
            "package": "rain.nodes.spark.pipeline.stages.LogisticRegression",
            "input": {
                "dataset": "DataFrame"
            },
            "output": {
                "model": "PipelineModel"
            },
            "parameter": [
                {
                    "name": "max_iter",
                    "type": null,
                    "is_mandatory": true,
                    "default_value": null,
                    "description": null
                },
                {
                    "name": "reg_param",
                    "type": null,
                    "is_mandatory": true,
                    "default_value": null,
                    "description": null
                }
            ],
            "methods": null,
            "tags": {
                "library": "PySpark",
                "type": "Estimator"
            },
            "name": "Logistic Regression",
            "description": "Represent a SparkNode that supports fitting traditional logistic regression model."
        },
        {
            "clazz": "SimpleKMeans",
            "package": "rain.nodes.sklearn.cluster.SimpleKMeans",
            "input": {
                "fit_dataset": "DataFrame",
                "predict_dataset": "DataFrame",
                "score_dataset": "DataFrame",
                "transform_dataset": "DataFrame"
            },
            "output": {
                "fitted_model": "BaseEstimator",
                "predictions": "DataFrame",
                "score_value": "float",
                "transformed_dataset": "DataFrame",
                "labels": "DataFrame"
            },
            "parameter": [
                {
                    "name": "execute",
                    "type": "list[str]",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Methods to execute with this clusterer, they can be: fit, predict, transform, score."
                },
                {
                    "name": "n_clusters",
                    "type": "int",
                    "is_mandatory": false,
                    "default_value": 8,
                    "description": "The number of clusters to form as well as the number of centroids to generate."
                }
            ],
            "methods": [
                "fit",
                "predict",
                "score",
                "transform"
            ],
            "tags": {
                "library": "Sklearn",
                "type": "CLusterer"
            },
            "name": "Simple K Means",
            "description": "A clusterer for the sklearn KMeans."
        },
        {
            "clazz": "SparkColumnSelector",
            "package": "rain.nodes.spark.data_wrangling.SparkColumnSelector",
            "input": {
                "dataset": "DataFrame"
            },
            "output": {
                "dataset": "DataFrame"
            },
            "parameter": [
                {
                    "name": "column_list",
                    "type": "List[str]",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "List of columns to select from the dataset"
                },
                {
                    "name": "filter_list",
                    "type": "List[str]",
                    "is_mandatory": false,
                    "default_value": [],
                    "description": "List of conditions used to filter the rows of the dataset"
                }
            ],
            "methods": null,
            "tags": {
                "library": "PySpark",
                "type": "Transformer"
            },
            "name": "Spark Column Selector",
            "description": "SparkColumnSelector manages filtering of rows, columns and values for a Spark DataFrame."
        },
        {
            "clazz": "PandasCSVWriter",
            "package": "rain.nodes.pandas.pandas_io.PandasCSVWriter",
            "input": {
                "dataset": "DataFrame"
            },
            "output": null,
            "parameter": [
                {
                    "name": "path",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Of the CSV file."
                },
                {
                    "name": "delim",
                    "type": "str",
                    "is_mandatory": false,
                    "default_value": ",",
                    "description": "Delimiter symbol of the CSV file."
                },
                {
                    "name": "include_rows",
                    "type": "bool",
                    "is_mandatory": false,
                    "default_value": true,
                    "description": "Whether to include rows indexes."
                },
                {
                    "name": "rows_column_label",
                    "type": "str",
                    "is_mandatory": false,
                    "default_value": null,
                    "description": "If rows indexes must be included you can give a name to its column."
                },
                {
                    "name": "include_columns",
                    "type": "bool",
                    "is_mandatory": false,
                    "default_value": true,
                    "description": "Whether to include column names."
                },
                {
                    "name": "columns",
                    "type": "list[str]",
                    "is_mandatory": false,
                    "default_value": null,
                    "description": "If column names must be included you can give names to them. The order is relevant."
                }
            ],
            "methods": null,
            "tags": {
                "library": "Pandas",
                "type": "Output"
            },
            "name": "Pandas CSV Writer",
            "description": "Writes a pandas DataFrame into a CSV file."
        },
        {
            "clazz": "HashingTF",
            "package": "rain.nodes.spark.pipeline.stages.HashingTF",
            "input": {
                "dataset": "DataFrame"
            },
            "output": {
                "dataset": "DataFrame"
            },
            "parameter": [
                {
                    "name": "in_col",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "The name of the input column"
                },
                {
                    "name": "out_col",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "The name of the output column"
                }
            ],
            "methods": null,
            "tags": {
                "library": "PySpark",
                "type": "Transformer"
            },
            "name": "Hashing TF",
            "description": "Represent a Spark HashingTF that maps a sequence of terms to their term frequencies using the hashing trick."
        },
        {
            "clazz": "PandasAddColumn",
            "package": "rain.nodes.pandas.transform_nodes.PandasAddColumn",
            "input": {
                "dataset": "DataFrame",
                "column": "Series"
            },
            "output": {
                "dataset": "DataFrame"
            },
            "parameter": [
                {
                    "name": "loc",
                    "type": "int",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Insertion index. Must verify 0 <= loc <= len(columns)"
                },
                {
                    "name": "col",
                    "type": "str",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "Label of the inserted column."
                }
            ],
            "methods": null,
            "tags": {
                "library": "Pandas",
                "type": "Transformer"
            },
            "name": "Pandas Add Column",
            "description": "Node used to add a column to a Pandas Dataframe starting from a given Pandas Series."
        },
        {
            "clazz": "SklearnPCA",
            "package": "rain.nodes.sklearn.decomposition.SklearnPCA",
            "input": {
                "fit_dataset": "DataFrame",
                "transform_dataset": "DataFrame",
                "score_dataset": "DataFrame"
            },
            "output": {
                "fitted_model": "BaseEstimator",
                "transformed_dataset": "DataFrame",
                "score_value": "float"
            },
            "parameter": [
                {
                    "name": "execute",
                    "type": "list[str]",
                    "is_mandatory": true,
                    "default_value": null,
                    "description": "List of methods to execute."
                },
                {
                    "name": "n_components",
                    "type": "int",
                    "is_mandatory": false,
                    "default_value": null,
                    "description": "Number of components to keep."
                },
                {
                    "name": "whiten",
                    "type": "bool",
                    "is_mandatory": false,
                    "default_value": false,
                    "description": "When True (False by default) the components_ vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances."
                },
                {
                    "name": "svd_solver",
                    "type": "{'auto', 'full', 'arpack', 'randomized'}",
                    "is_mandatory": false,
                    "default_value": "auto",
                    "description": "Svd solver."
                },
                {
                    "name": "tol",
                    "type": "float",
                    "is_mandatory": false,
                    "default_value": 0.0,
                    "description": "Tolerance for singular values computed by svd_solver == \u2018arpack\u2019. Must be positive."
                },
                {
                    "name": "iterated_power",
                    "type": "int",
                    "is_mandatory": false,
                    "default_value": "auto",
                    "description": "Number of iterations for the power method computed by svd_solver == \u2018randomized\u2019. Must be positive."
                },
                {
                    "name": "random_state",
                    "type": "int",
                    "is_mandatory": false,
                    "default_value": null,
                    "description": "Used when the \u2018arpack\u2019 or \u2018randomized\u2019 solvers are used. Pass an int for reproducible results across multiple function calls."
                }
            ],
            "methods": [
                "fit",
                "transform",
                "score"
            ],
            "tags": {
                "library": "Sklearn",
                "type": "Estimator"
            },
            "name": "Sklearn PCA",
            "description": "Node representation of a sklearn PCA estimator."
        }
    ],
    "dependencies": [
        "pandas~=1.3.0",
        "sklearn~=0.0",
        "pyspark~=3.1.2",
        "pymongo~=3.12.0"
    ]
}

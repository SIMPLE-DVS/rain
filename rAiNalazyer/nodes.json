[
    {
        "clazz": "MongoCSVReader",
        "package": "rain.nodes.mongodb.database_io.MongoCSVReader",
        "input": null,
        "output": {
            "dataset": "DataFrame"
        },
        "parameter": [
            {
                "name": "connection",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "Hostname or IP address or Unix domain socket path of a single MongoDB instance to connect to, or a mongodb URI"
            },
            {
                "name": "db",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "Name of the database to connect to."
            },
            {
                "name": "coll",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "Name of the collection to connect to."
            },
            {
                "name": "filter",
                "type": "dict",
                "is_mandatory": false,
                "default_value": null,
                "description": "A SON object specifying elements which must be present for a document to be included in the result set"
            },
            {
                "name": "projection",
                "type": "dict",
                "is_mandatory": false,
                "default_value": null,
                "description": "A dict to exclude fields from the result (e.g. projection={'_id': False})"
            }
        ],
        "methods": null,
        "tags": {
            "library": "Other",
            "type": "Input"
        },
        "name": "Mongo CSV Reader",
        "description": "Read a Pandas Dataframe from a MongoDB collection."
    },
    {
        "clazz": "SparkColumnSelector",
        "package": "rain.nodes.spark.data_wrangling.SparkColumnSelector",
        "input": {
            "dataset": "DataFrame"
        },
        "output": {
            "dataset": "DataFrame"
        },
        "parameter": [
            {
                "name": "column_list",
                "type": "List[str]",
                "is_mandatory": true,
                "default_value": null,
                "description": "List of columns to select from the dataset"
            },
            {
                "name": "filter_list",
                "type": "List[str]",
                "is_mandatory": false,
                "default_value": [],
                "description": "List of conditions used to filter the rows of the dataset"
            }
        ],
        "methods": null,
        "tags": {
            "library": "Spark",
            "type": "Transformer"
        },
        "name": "Spark Column Selector",
        "description": "SparkColumnSelector manages filtering of rows, columns and values for a Spark DataFrame."
    },
    {
        "clazz": "DaviesBouldinScore",
        "package": "rain.nodes.sklearn.functions.DaviesBouldinScore",
        "input": {
            "samples_dataset": "DataFrame",
            "labels": "DataFrame"
        },
        "output": {
            "score": "float"
        },
        "parameter": [],
        "methods": [],
        "tags": {
            "library": "Sklearn",
            "type": "Metrics"
        },
        "name": "Davies Bouldin Score",
        "description": "Computes the Davies-Bouldin score. The score is defined as the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of within-cluster distances to between-cluster distances. Thus, clusters which are farther apart and less dispersed will result in a better score. The minimum score is zero, with lower values indicating better clustering."
    },
    {
        "clazz": "PandasCSVWriter",
        "package": "rain.nodes.pandas.pandas_io.PandasCSVWriter",
        "input": {
            "dataset": "DataFrame"
        },
        "output": null,
        "parameter": [
            {
                "name": "path",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "Of the CSV file."
            },
            {
                "name": "delim",
                "type": "str",
                "is_mandatory": false,
                "default_value": ",",
                "description": "Delimiter symbol of the CSV file."
            },
            {
                "name": "include_rows",
                "type": "bool",
                "is_mandatory": false,
                "default_value": true,
                "description": "Whether to include rows indexes."
            },
            {
                "name": "rows_column_label",
                "type": "str",
                "is_mandatory": false,
                "default_value": null,
                "description": "If rows indexes must be included you can give a name to its column."
            },
            {
                "name": "include_columns",
                "type": "bool",
                "is_mandatory": false,
                "default_value": true,
                "description": "Whether to include column names."
            },
            {
                "name": "columns",
                "type": "list[str]",
                "is_mandatory": false,
                "default_value": null,
                "description": "If column names must be included you can give names to them. The order is relevant."
            }
        ],
        "methods": null,
        "tags": {
            "library": "Pandas",
            "type": "Output"
        },
        "name": "Pandas CSV Writer",
        "description": "Writes a pandas DataFrame into a CSV file."
    },
    {
        "clazz": "LogisticRegression",
        "package": "rain.nodes.spark.pipeline.stages.LogisticRegression",
        "input": {
            "dataset": "DataFrame"
        },
        "output": {
            "model": "PipelineModel"
        },
        "parameter": [
            {
                "name": "max_iter",
                "type": null,
                "is_mandatory": true,
                "default_value": null,
                "description": null
            },
            {
                "name": "reg_param",
                "type": null,
                "is_mandatory": true,
                "default_value": null,
                "description": null
            }
        ],
        "methods": null,
        "tags": {
            "library": "Spark",
            "type": "Estimator"
        },
        "name": "Logistic Regression",
        "description": "Represent a SparkNode that supports fitting traditional logistic regression model."
    },
    {
        "clazz": "HashingTF",
        "package": "rain.nodes.spark.pipeline.stages.HashingTF",
        "input": {
            "dataset": "DataFrame"
        },
        "output": {
            "dataset": "DataFrame"
        },
        "parameter": [
            {
                "name": "in_col",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "The name of the input column"
            },
            {
                "name": "out_col",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "The name of the output column"
            }
        ],
        "methods": null,
        "tags": {
            "library": "Spark",
            "type": "Transformer"
        },
        "name": "Hashing TF",
        "description": "Represent a Spark HashingTF that maps a sequence of terms to their term frequencies using the hashing trick."
    },
    {
        "clazz": "PandasSelectRows",
        "package": "rain.nodes.pandas.transform_nodes.PandasSelectRows",
        "input": {
            "dataset": "DataFrame"
        },
        "output": {
            "selection": "Series"
        },
        "parameter": [
            {
                "name": "select_nan",
                "type": "bool",
                "is_mandatory": false,
                "default_value": false,
                "description": "Select rows with at least one NaN value."
            },
            {
                "name": "conditions",
                "type": "List[str]",
                "is_mandatory": false,
                "default_value": null,
                "description": "List of conditions to select rows."
            }
        ],
        "methods": null,
        "tags": {
            "library": "Pandas",
            "type": "Transformer"
        },
        "name": "Pandas Select Rows",
        "description": "PandasSelectRows manages selection of rows, which can later be filtered or deleted."
    },
    {
        "clazz": "PandasDropNan",
        "package": "rain.nodes.pandas.transform_nodes.PandasDropNan",
        "input": {
            "dataset": "DataFrame"
        },
        "output": {
            "dataset": "DataFrame"
        },
        "parameter": [
            {
                "name": "axis",
                "type": "{'rows', 'columns'}",
                "is_mandatory": false,
                "default_value": "rows",
                "description": "The axis from where to remove the nan values."
            },
            {
                "name": "how",
                "type": "{'any', 'all'}",
                "is_mandatory": false,
                "default_value": "any",
                "description": "Whether to remove a row or a column which either contains any nan value or contains all nan values."
            }
        ],
        "methods": null,
        "tags": {
            "library": "Pandas",
            "type": "Transformer"
        },
        "name": "Pandas Drop Nan",
        "description": "Drops rows or columns that either only contains a nan or that has all nan values."
    },
    {
        "clazz": "SparkPipelineNode",
        "package": "rain.nodes.spark.pipeline.spark_pipeline.SparkPipelineNode",
        "input": {
            "dataset": "DataFrame"
        },
        "output": {
            "model": "PipelineModel"
        },
        "parameter": [
            {
                "name": "stages",
                "type": null,
                "is_mandatory": true,
                "default_value": null,
                "description": null
            }
        ],
        "methods": null,
        "tags": {
            "library": "Spark",
            "type": "Estimator"
        },
        "name": "Spark Pipeline Node",
        "description": "Represent a Spark Pipeline consisting of SparkNode (stages)"
    },
    {
        "clazz": "PandasFilterRows",
        "package": "rain.nodes.pandas.transform_nodes.PandasFilterRows",
        "input": {
            "dataset": "DataFrame",
            "selected_rows": "Series"
        },
        "output": {
            "dataset": "DataFrame"
        },
        "parameter": [],
        "methods": null,
        "tags": {
            "library": "Pandas",
            "type": "Transformer"
        },
        "name": "Pandas Filter Rows",
        "description": "PandasFilterRows manages filtering of rows that have been previously selected."
    },
    {
        "clazz": "PandasReplaceColumn",
        "package": "rain.nodes.pandas.transform_nodes.PandasReplaceColumn",
        "input": {
            "column": "Series"
        },
        "output": {
            "column": "Series"
        },
        "parameter": [
            {
                "name": "first_value",
                "type": "Any",
                "is_mandatory": true,
                "default_value": null,
                "description": "Value used when the condition is True."
            },
            {
                "name": "second_value",
                "type": "Any",
                "is_mandatory": true,
                "default_value": null,
                "description": "Value used when the condition is True."
            }
        ],
        "methods": null,
        "tags": {
            "library": "Pandas",
            "type": "Transformer"
        },
        "name": "Pandas Replace Column",
        "description": "Node used to replace the boolean values of a Pandas Series with other values given by the user."
    },
    {
        "clazz": "TrainTestDatasetSplit",
        "package": "rain.nodes.sklearn.functions.TrainTestDatasetSplit",
        "input": {
            "dataset": "DataFrame"
        },
        "output": {
            "train_dataset": "DataFrame",
            "test_dataset": "DataFrame"
        },
        "parameter": [
            {
                "name": "test_size",
                "type": null,
                "is_mandatory": false,
                "default_value": null,
                "description": null
            },
            {
                "name": "train_size",
                "type": null,
                "is_mandatory": false,
                "default_value": null,
                "description": null
            },
            {
                "name": "random_state",
                "type": null,
                "is_mandatory": false,
                "default_value": null,
                "description": null
            },
            {
                "name": "shuffle",
                "type": null,
                "is_mandatory": false,
                "default_value": true,
                "description": null
            }
        ],
        "methods": [],
        "tags": {
            "library": "Sklearn",
            "type": "Transformer"
        },
        "name": "Train Test Dataset Split",
        "description": ""
    },
    {
        "clazz": "MongoCSVWriter",
        "package": "rain.nodes.mongodb.database_io.MongoCSVWriter",
        "input": {
            "dataset": "DataFrame"
        },
        "output": null,
        "parameter": [
            {
                "name": "connection",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "Hostname or IP address or Unix domain socket path of a single MongoDB instance to connect to, or a mongodb URI"
            },
            {
                "name": "db",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "Name of the database to connect to."
            },
            {
                "name": "coll",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "Name of the collection to connect to."
            }
        ],
        "methods": null,
        "tags": {
            "library": "Other",
            "type": "Output"
        },
        "name": "Mongo CSV Writer",
        "description": "Write a Pandas Dataframe into a MongoDB collection."
    },
    {
        "clazz": "CustomNode",
        "package": "rain.nodes.custom.custom.CustomNode",
        "input": {},
        "output": {},
        "parameter": [
            {
                "name": "use_function",
                "type": null,
                "is_mandatory": true,
                "default_value": null,
                "description": null
            },
            {
                "name": "kwargs",
                "type": null,
                "is_mandatory": true,
                "default_value": null,
                "description": null
            }
        ],
        "methods": null,
        "tags": {
            "library": "Custom",
            "type": "Custom"
        },
        "name": "Custom Node",
        "description": "A node that can contain user-defined Python code."
    },
    {
        "clazz": "SklearnLinearSVC",
        "package": "rain.nodes.sklearn.svm.SklearnLinearSVC",
        "input": {
            "fit_dataset": "DataFrame",
            "predict_dataset": "DataFrame",
            "score_dataset": "DataFrame",
            "fit_targets": "DataFrame",
            "score_targets": "DataFrame"
        },
        "output": {
            "fitted_model": "BaseEstimator",
            "predictions": "DataFrame",
            "score_value": "float"
        },
        "parameter": [
            {
                "name": "execute",
                "type": null,
                "is_mandatory": true,
                "default_value": null,
                "description": null
            },
            {
                "name": "penalty",
                "type": null,
                "is_mandatory": false,
                "default_value": "l2",
                "description": null
            },
            {
                "name": "loss",
                "type": null,
                "is_mandatory": false,
                "default_value": "squared_hinge",
                "description": null
            },
            {
                "name": "dual",
                "type": null,
                "is_mandatory": false,
                "default_value": true,
                "description": null
            },
            {
                "name": "tol",
                "type": null,
                "is_mandatory": false,
                "default_value": 0.0001,
                "description": null
            },
            {
                "name": "C",
                "type": null,
                "is_mandatory": false,
                "default_value": 1.0,
                "description": null
            },
            {
                "name": "multi_class",
                "type": null,
                "is_mandatory": false,
                "default_value": "ovr",
                "description": null
            },
            {
                "name": "fit_intercept",
                "type": null,
                "is_mandatory": false,
                "default_value": true,
                "description": null
            },
            {
                "name": "intercept_scaling",
                "type": null,
                "is_mandatory": false,
                "default_value": 1,
                "description": null
            },
            {
                "name": "class_weight",
                "type": null,
                "is_mandatory": false,
                "default_value": null,
                "description": null
            },
            {
                "name": "verbose",
                "type": null,
                "is_mandatory": false,
                "default_value": 0,
                "description": null
            },
            {
                "name": "random_state",
                "type": null,
                "is_mandatory": false,
                "default_value": null,
                "description": null
            },
            {
                "name": "max_iter",
                "type": null,
                "is_mandatory": false,
                "default_value": 1000,
                "description": null
            }
        ],
        "methods": [
            "fit",
            "predict",
            "score"
        ],
        "tags": {
            "library": "Sklearn",
            "type": "Classifier"
        },
        "name": "Sklearn Linear SVC",
        "description": ""
    },
    {
        "clazz": "SparkSaveDataset",
        "package": "rain.nodes.spark.spark_output.SparkSaveDataset",
        "input": {
            "dataset": "DataFrame"
        },
        "output": null,
        "parameter": [
            {
                "name": "path",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "String representing the path where to save the dataset"
            },
            {
                "name": "index",
                "type": "bool",
                "is_mandatory": false,
                "default_value": true,
                "description": "String representing the path where to save the dataset"
            }
        ],
        "methods": null,
        "tags": {
            "library": "Spark",
            "type": "Output"
        },
        "name": "Spark Save Dataset",
        "description": "Save a Spark Dataframe in a .csv format"
    },
    {
        "clazz": "SparkSaveModel",
        "package": "rain.nodes.spark.spark_output.SparkSaveModel",
        "input": {
            "model": "PipelineModel"
        },
        "output": null,
        "parameter": [
            {
                "name": "path",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "String representing the path where to save the model"
            }
        ],
        "methods": null,
        "tags": {
            "library": "Spark",
            "type": "Output"
        },
        "name": "Spark Save Model",
        "description": "Save a trained PipelineModel"
    },
    {
        "clazz": "PandasAddColumn",
        "package": "rain.nodes.pandas.transform_nodes.PandasAddColumn",
        "input": {
            "dataset": "DataFrame",
            "column": "Series"
        },
        "output": {
            "dataset": "DataFrame"
        },
        "parameter": [
            {
                "name": "loc",
                "type": "int",
                "is_mandatory": true,
                "default_value": null,
                "description": "Insertion index. Must verify 0 <= loc <= len(columns)"
            },
            {
                "name": "col",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "Label of the inserted column."
            }
        ],
        "methods": null,
        "tags": {
            "library": "Pandas",
            "type": "Transformer"
        },
        "name": "Pandas Add Column",
        "description": "Node used to add a column to a Pandas Dataframe starting from a given Pandas Series."
    },
    {
        "clazz": "SparkModelLoader",
        "package": "rain.nodes.spark.spark_input.SparkModelLoader",
        "input": null,
        "output": {
            "model": "PipelineModel"
        },
        "parameter": [
            {
                "name": "path",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "Path of the csv file."
            }
        ],
        "methods": null,
        "tags": {
            "library": "Spark",
            "type": "Input"
        },
        "name": "Spark Model Loader",
        "description": "Loads a file as a Spark Model."
    },
    {
        "clazz": "PandasColumnsFiltering",
        "package": "rain.nodes.pandas.transform_nodes.PandasColumnsFiltering",
        "input": {
            "dataset": "DataFrame"
        },
        "output": {
            "dataset": "DataFrame"
        },
        "parameter": [
            {
                "name": "column_indexes",
                "type": "List[int]",
                "is_mandatory": false,
                "default_value": null,
                "description": "Filters the dataset selecting the given indexes. Uses the pandas iloc function."
            },
            {
                "name": "column_names",
                "type": "List[str]",
                "is_mandatory": false,
                "default_value": null,
                "description": "Filters the dataset selecting the given column labels. Uses the pandas filter function."
            },
            {
                "name": "columns_like",
                "type": "str",
                "is_mandatory": false,
                "default_value": null,
                "description": "Keep columns for which the given string is a substring of the column label."
            },
            {
                "name": "columns_regex",
                "type": "str",
                "is_mandatory": false,
                "default_value": null,
                "description": "Keep columns for which column labels match a given pattern."
            },
            {
                "name": "columns_range",
                "type": "Tuple[int, int]",
                "is_mandatory": false,
                "default_value": null,
                "description": "Keep columns for which index falls withing the given range (from, to (excluded))."
            },
            {
                "name": "columns_type",
                "type": "str or List[str]",
                "is_mandatory": false,
                "default_value": null,
                "description": "Type to assign to columns. It can be either a string, meaning that it will try to apply the chosen type to all the columns, or a list of strings, one for each column, meaning that it will try to assign a chosen type to each column in order."
            }
        ],
        "methods": null,
        "tags": {
            "library": "Pandas",
            "type": "Transformer"
        },
        "name": "Pandas Columns Filtering",
        "description": "PandasColumnsFiltering manages filtering of columns. This node gives access to several functionalities such as: - select columns by their indexes; - select columns by their names (labels); - select columns containing a substring in their names; - select columns that match a regex; - select columns in a range of indexes; - assign a type to a column. Every parameter but 'columns_type' is mutually exclusive, meaning that only one can be used."
    },
    {
        "clazz": "PandasCSVLoader",
        "package": "rain.nodes.pandas.pandas_io.PandasCSVLoader",
        "input": null,
        "output": {
            "dataset": "DataFrame"
        },
        "parameter": [
            {
                "name": "path",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "Of the CSV file."
            },
            {
                "name": "delim",
                "type": "str",
                "is_mandatory": false,
                "default_value": ",",
                "description": "Delimiter symbol of the CSV file."
            }
        ],
        "methods": null,
        "tags": {
            "library": "Pandas",
            "type": "Input"
        },
        "name": "Pandas CSV Loader",
        "description": "Loads a pandas DataFrame from a CSV file."
    },
    {
        "clazz": "Tokenizer",
        "package": "rain.nodes.spark.pipeline.stages.Tokenizer",
        "input": {
            "dataset": "DataFrame"
        },
        "output": {
            "dataset": "DataFrame"
        },
        "parameter": [
            {
                "name": "in_col",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "The name of the input column"
            },
            {
                "name": "out_col",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "The name of the output column"
            }
        ],
        "methods": null,
        "tags": {
            "library": "Spark",
            "type": "Transformer"
        },
        "name": "Tokenizer",
        "description": "Represent a Spark Tokenizer used to split text in individual term."
    },
    {
        "clazz": "PandasPivot",
        "package": "rain.nodes.pandas.transform_nodes.PandasPivot",
        "input": {
            "dataset": "DataFrame"
        },
        "output": {
            "dataset": "DataFrame"
        },
        "parameter": [
            {
                "name": "rows",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "Name of the column whose values will be the rows of the pivot."
            },
            {
                "name": "columns",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "Name of the column whose values will be the columns of the pivot."
            },
            {
                "name": "values",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "Name of the column whose values will be the values of the pivot."
            },
            {
                "name": "aggfunc",
                "type": "str",
                "is_mandatory": false,
                "default_value": "mean",
                "description": "Function to use for the aggregation."
            },
            {
                "name": "fill_value",
                "type": "int",
                "is_mandatory": false,
                "default_value": 0,
                "description": "Value to replace missing values with."
            },
            {
                "name": "dropna",
                "type": "bool",
                "is_mandatory": false,
                "default_value": true,
                "description": "Do not include columns whose entries are all NaN."
            },
            {
                "name": "sort",
                "type": "bool",
                "is_mandatory": false,
                "default_value": true,
                "description": "Specifies if the result should be sorted."
            }
        ],
        "methods": null,
        "tags": {
            "library": "Pandas",
            "type": "Transformer"
        },
        "name": "Pandas Pivot",
        "description": "Transforms a DataFrame into a Pivot from the given rows, columns and values."
    },
    {
        "clazz": "PandasRenameColumn",
        "package": "rain.nodes.pandas.transform_nodes.PandasRenameColumn",
        "input": {
            "dataset": "DataFrame"
        },
        "output": {
            "dataset": "DataFrame"
        },
        "parameter": [
            {
                "name": "columns",
                "type": "list[str]",
                "is_mandatory": true,
                "default_value": null,
                "description": "Column names to assign to the DataFrame. The order is relevant."
            }
        ],
        "methods": null,
        "tags": {
            "library": "Pandas",
            "type": "Transformer"
        },
        "name": "Pandas Rename Column",
        "description": "Sets column names for a pandas DataFrame."
    },
    {
        "clazz": "SparkSplitDataset",
        "package": "rain.nodes.spark.data_wrangling.SparkSplitDataset",
        "input": {
            "dataset": "DataFrame"
        },
        "output": {
            "dataset": "DataFrame",
            "train_dataset": "DataFrame",
            "test_dataset": "DataFrame"
        },
        "parameter": [
            {
                "name": "train",
                "type": "float",
                "is_mandatory": true,
                "default_value": null,
                "description": "Percentage of the dataset to split into a train dataset."
            },
            {
                "name": "test",
                "type": "float",
                "is_mandatory": true,
                "default_value": null,
                "description": "Percentage of the dataset to split into a test dataset."
            }
        ],
        "methods": null,
        "tags": {
            "library": "Spark",
            "type": "Transformer"
        },
        "name": "Spark Split Dataset",
        "description": "Splits a Spark DataFrame in two DataFrames, train and test."
    },
    {
        "clazz": "SklearnPCA",
        "package": "rain.nodes.sklearn.decomposition.SklearnPCA",
        "input": {
            "fit_dataset": "DataFrame",
            "transform_dataset": "DataFrame",
            "score_dataset": "DataFrame"
        },
        "output": {
            "fitted_model": "BaseEstimator",
            "transformed_dataset": "DataFrame",
            "score_value": "float"
        },
        "parameter": [
            {
                "name": "execute",
                "type": "list[str]",
                "is_mandatory": true,
                "default_value": null,
                "description": "List of methods to execute."
            },
            {
                "name": "n_components",
                "type": "int",
                "is_mandatory": false,
                "default_value": null,
                "description": "Number of components to keep."
            },
            {
                "name": "whiten",
                "type": "bool",
                "is_mandatory": false,
                "default_value": false,
                "description": "When True (False by default) the components_ vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances."
            },
            {
                "name": "svd_solver",
                "type": "{'auto', 'full', 'arpack', 'randomized'}",
                "is_mandatory": false,
                "default_value": "auto",
                "description": "Svd solver."
            },
            {
                "name": "tol",
                "type": "float",
                "is_mandatory": false,
                "default_value": 0.0,
                "description": "Tolerance for singular values computed by svd_solver == \u2018arpack\u2019. Must be positive."
            },
            {
                "name": "iterated_power",
                "type": "int",
                "is_mandatory": false,
                "default_value": "auto",
                "description": "Number of iterations for the power method computed by svd_solver == \u2018randomized\u2019. Must be positive."
            },
            {
                "name": "random_state",
                "type": "int",
                "is_mandatory": false,
                "default_value": null,
                "description": "Used when the \u2018arpack\u2019 or \u2018randomized\u2019 solvers are used. Pass an int for reproducible results across multiple function calls."
            }
        ],
        "methods": [
            "fit",
            "transform",
            "score"
        ],
        "tags": {
            "library": "Sklearn",
            "type": "Estimator"
        },
        "name": "Sklearn PCA",
        "description": "Node representation of a sklearn PCA estimator."
    },
    {
        "clazz": "PandasSequence",
        "package": "rain.nodes.pandas.transform_nodes.PandasSequence",
        "input": {
            "dataset": "DataFrame"
        },
        "output": {
            "dataset": "DataFrame"
        },
        "parameter": [
            {
                "name": "stages",
                "type": "list of PandasTransformer",
                "is_mandatory": true,
                "default_value": null,
                "description": "ordered in an execution sequence. They must all be PandasNodes, hence have a 'dataset' variable used for input and output."
            }
        ],
        "methods": null,
        "tags": {
            "library": "Pandas",
            "type": "Transformer"
        },
        "name": "Pandas Sequence",
        "description": "PandasSequence wraps a list of nodes that must be executed in sequence into a single node. Intermediate values are passed along the chain using the 'dataset' variable, hence only PandasNodes can be used within a sequence."
    },
    {
        "clazz": "SparkCSVLoader",
        "package": "rain.nodes.spark.spark_input.SparkCSVLoader",
        "input": null,
        "output": {
            "dataset": "DataFrame"
        },
        "parameter": [
            {
                "name": "path",
                "type": "str",
                "is_mandatory": true,
                "default_value": null,
                "description": "Path of the csv file."
            },
            {
                "name": "header",
                "type": "bool",
                "is_mandatory": false,
                "default_value": false,
                "description": "Uses the first line as names of columns."
            },
            {
                "name": "schema",
                "type": "bool",
                "is_mandatory": false,
                "default_value": false,
                "description": "Infers the input schema automatically from data. It requires one extra pass over the data."
            }
        ],
        "methods": null,
        "tags": {
            "library": "Spark",
            "type": "Input"
        },
        "name": "Spark CSV Loader",
        "description": "Loads a CSV file as a Spark DataFrame."
    },
    {
        "clazz": "IrisDatasetLoader",
        "package": "rain.nodes.sklearn.loaders.IrisDatasetLoader",
        "input": null,
        "output": {
            "dataset": "DataFrame",
            "target": "DataFrame"
        },
        "parameter": [
            {
                "name": "separate_target",
                "type": null,
                "is_mandatory": false,
                "default_value": false,
                "description": null
            }
        ],
        "methods": null,
        "tags": {
            "library": "Pandas",
            "type": "Input"
        },
        "name": "Iris Dataset Loader",
        "description": "Loads the iris dataset as a pandas DataFrame."
    },
    {
        "clazz": "TrainTestSampleTargetSplit",
        "package": "rain.nodes.sklearn.functions.TrainTestSampleTargetSplit",
        "input": {
            "sample_dataset": "DataFrame",
            "target_dataset": "DataFrame"
        },
        "output": {
            "sample_train_dataset": "DataFrame",
            "sample_test_dataset": "DataFrame",
            "target_train_dataset": "DataFrame",
            "target_test_dataset": "DataFrame"
        },
        "parameter": [
            {
                "name": "test_size",
                "type": null,
                "is_mandatory": false,
                "default_value": null,
                "description": null
            },
            {
                "name": "train_size",
                "type": null,
                "is_mandatory": false,
                "default_value": null,
                "description": null
            },
            {
                "name": "random_state",
                "type": null,
                "is_mandatory": false,
                "default_value": null,
                "description": null
            },
            {
                "name": "shuffle",
                "type": null,
                "is_mandatory": false,
                "default_value": true,
                "description": null
            }
        ],
        "methods": [],
        "tags": {
            "library": "Sklearn",
            "type": "Transformer"
        },
        "name": "Train Test Sample Target Split",
        "description": ""
    },
    {
        "clazz": "SimpleKMeans",
        "package": "rain.nodes.sklearn.cluster.SimpleKMeans",
        "input": {
            "fit_dataset": "DataFrame",
            "predict_dataset": "DataFrame",
            "score_dataset": "DataFrame",
            "transform_dataset": "DataFrame"
        },
        "output": {
            "fitted_model": "BaseEstimator",
            "predictions": "DataFrame",
            "score_value": "float",
            "transformed_dataset": "DataFrame",
            "labels": "DataFrame"
        },
        "parameter": [
            {
                "name": "execute",
                "type": "list[str]",
                "is_mandatory": true,
                "default_value": null,
                "description": "Methods to execute with this clusterer, they can be: fit, predict, transform, score."
            },
            {
                "name": "n_clusters",
                "type": "int",
                "is_mandatory": false,
                "default_value": 8,
                "description": "The number of clusters to form as well as the number of centroids to generate."
            }
        ],
        "methods": [
            "fit",
            "predict",
            "score",
            "transform"
        ],
        "tags": {
            "library": "Sklearn",
            "type": "CLusterer"
        },
        "name": "Simple K Means",
        "description": "A clusterer for the sklearn KMeans."
    }
]

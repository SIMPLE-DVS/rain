{
  "nodes": [
    {
      "clazz": "CustomNode",
      "package": "rain.nodes.custom.custom.CustomNode",
      "input": {},
      "output": {},
      "parameter": [
        {
          "name": "use_function",
          "type": null,
          "is_mandatory": true,
          "default_value": null,
          "description": null
        },
        {
          "name": "kwargs",
          "type": null,
          "is_mandatory": true,
          "default_value": null,
          "description": null
        }
      ],
      "methods": null,
      "tags": {
        "library": "Base",
        "type": "Custom"
      },
      "description": "A node that can contain user-defined Python code."
    },
    {
      "clazz": "MongoCSVReader",
      "package": "rain.nodes.mongodb.database_io.MongoCSVReader",
      "input": null,
      "output": {
        "dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "connection",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "Hostname or IP address or Unix domain socket path of a single MongoDB instance to connect to, or a mongodb URI"
        },
        {
          "name": "db",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "Name of the database to connect to."
        },
        {
          "name": "coll",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "Name of the collection to connect to."
        },
        {
          "name": "filter",
          "type": "dict",
          "is_mandatory": false,
          "default_value": null,
          "description": "A SON object specifying elements which must be present for a document to be included in the result set"
        },
        {
          "name": "projection",
          "type": "dict",
          "is_mandatory": false,
          "default_value": null,
          "description": "A dict to exclude fields from the result (e.g. projection={'_id': False})"
        }
      ],
      "methods": null,
      "tags": {
        "library": "PyMongo",
        "type": "Input"
      },
      "description": "Read a Pandas Dataframe from a MongoDB collection."
    },
    {
      "clazz": "MongoCSVWriter",
      "package": "rain.nodes.mongodb.database_io.MongoCSVWriter",
      "input": {
        "dataset": "DataFrame"
      },
      "output": null,
      "parameter": [
        {
          "name": "connection",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "Hostname or IP address or Unix domain socket path of a single MongoDB instance to connect to, or a mongodb URI"
        },
        {
          "name": "db",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "Name of the database to connect to."
        },
        {
          "name": "coll",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "Name of the collection to connect to."
        }
      ],
      "methods": null,
      "tags": {
        "library": "PyMongo",
        "type": "Output"
      },
      "description": "Write a Pandas Dataframe into a MongoDB collection."
    },
    {
      "clazz": "PickleModelLoader",
      "package": "rain.nodes.pandas.model_io.PickleModelLoader",
      "input": null,
      "output": {
        "model": "pickle"
      },
      "parameter": [
        {
          "name": "path",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "The path of the stored object/model."
        }
      ],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Input"
      },
      "description": "Node that loads a given object, for instance a trained model, stored in pickle format."
    },
    {
      "clazz": "PickleModelWriter",
      "package": "rain.nodes.pandas.model_io.PickleModelWriter",
      "input": {
        "model": "pickle"
      },
      "output": null,
      "parameter": [
        {
          "name": "path",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "The path/filename where to store the object/model."
        }
      ],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Output"
      },
      "description": "Node that stores a given object, for instance a trained model, in pickle format."
    },
    {
      "clazz": "PandasCSVLoader",
      "package": "rain.nodes.pandas.pandas_io.PandasCSVLoader",
      "input": null,
      "output": {
        "dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "path",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "Of the CSV file."
        },
        {
          "name": "delim",
          "type": "str",
          "is_mandatory": false,
          "default_value": ",",
          "description": "Delimiter symbol of the CSV file."
        },
        {
          "name": "index_col",
          "type": "str",
          "is_mandatory": false,
          "default_value": null,
          "description": "Column to use as the row labels of the DataFrame, given as string name"
        }
      ],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Input"
      },
      "description": "Loads a pandas DataFrame from a CSV file."
    },
    {
      "clazz": "PandasCSVWriter",
      "package": "rain.nodes.pandas.pandas_io.PandasCSVWriter",
      "input": {
        "dataset": "DataFrame"
      },
      "output": null,
      "parameter": [
        {
          "name": "path",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "Of the CSV file."
        },
        {
          "name": "delim",
          "type": "str",
          "is_mandatory": false,
          "default_value": ",",
          "description": "Delimiter symbol of the CSV file."
        },
        {
          "name": "include_rows",
          "type": "bool",
          "is_mandatory": false,
          "default_value": true,
          "description": "Whether to include rows indexes."
        },
        {
          "name": "rows_column_label",
          "type": "str",
          "is_mandatory": false,
          "default_value": null,
          "description": "If rows indexes must be included you can give a name to its column."
        },
        {
          "name": "include_columns",
          "type": "bool",
          "is_mandatory": false,
          "default_value": true,
          "description": "Whether to include column names."
        },
        {
          "name": "columns",
          "type": "list[str]",
          "is_mandatory": false,
          "default_value": null,
          "description": "If column names must be included you can give names to them. The order is relevant."
        }
      ],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Output"
      },
      "description": "Writes a pandas DataFrame into a CSV file."
    },
    {
      "clazz": "PandasAddColumn",
      "package": "rain.nodes.pandas.transform_nodes.PandasAddColumn",
      "input": {
        "dataset": "DataFrame",
        "column": "Series"
      },
      "output": {
        "dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "loc",
          "type": "int",
          "is_mandatory": true,
          "default_value": null,
          "description": "Insertion index. Must verify 0 <= loc <= len(columns)"
        },
        {
          "name": "col",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "Label of the inserted column."
        }
      ],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Transformer"
      },
      "description": "Node used to add a column to a Pandas Dataframe starting from a given Pandas Series."
    },
    {
      "clazz": "PandasColumnsFiltering",
      "package": "rain.nodes.pandas.transform_nodes.PandasColumnsFiltering",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "column_indexes",
          "type": "List[int]",
          "is_mandatory": false,
          "default_value": null,
          "description": "Filters the dataset selecting the given indexes. Uses the pandas iloc function."
        },
        {
          "name": "column_names",
          "type": "List[str]",
          "is_mandatory": false,
          "default_value": null,
          "description": "Filters the dataset selecting the given column labels. Uses the pandas filter function."
        },
        {
          "name": "columns_like",
          "type": "str",
          "is_mandatory": false,
          "default_value": null,
          "description": "Keep columns for which the given string is a substring of the column label."
        },
        {
          "name": "columns_regex",
          "type": "str",
          "is_mandatory": false,
          "default_value": null,
          "description": "Keep columns for which column labels match a given pattern."
        },
        {
          "name": "columns_range",
          "type": "Tuple[int, int]",
          "is_mandatory": false,
          "default_value": null,
          "description": "Keep columns for which index falls withing the given range (from, to (excluded))."
        },
        {
          "name": "columns_type",
          "type": "str or List[str]",
          "is_mandatory": false,
          "default_value": null,
          "description": "Type to assign to columns. It can be either a string, meaning that it will try to apply the chosen type to all the columns, or a list of strings, one for each column, meaning that it will try to assign a chosen type to each column in order."
        }
      ],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Transformer"
      },
      "description": "PandasColumnsFiltering manages filtering of columns. This node gives access to several functionalities such as: - select columns by their indexes; - select columns by their names (labels); - select columns containing a substring in their names; - select columns that match a regex; - select columns in a range of indexes; - assign a type to a column. Every parameter but 'columns_type' is mutually exclusive, meaning that only one can be used."
    },
    {
      "clazz": "PandasDropNan",
      "package": "rain.nodes.pandas.transform_nodes.PandasDropNan",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "axis",
          "type": "{rows, columns}",
          "is_mandatory": false,
          "default_value": "rows",
          "description": "The axis from where to remove the nan values."
        },
        {
          "name": "how",
          "type": "{any, all}",
          "is_mandatory": false,
          "default_value": "any",
          "description": "Whether to remove a row or a column which either contains any nan value or contains all nan values."
        }
      ],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Transformer"
      },
      "description": "Drops rows or columns that either has at least a NaN value or that has all NaN values."
    },
    {
      "clazz": "PandasFilterRows",
      "package": "rain.nodes.pandas.transform_nodes.PandasFilterRows",
      "input": {
        "dataset": "DataFrame",
        "selected_rows": "Series"
      },
      "output": {
        "dataset": "DataFrame"
      },
      "parameter": [],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Transformer"
      },
      "description": "PandasFilterRows manages filtering of rows that have been previously selected."
    },
    {
      "clazz": "PandasGroupBy",
      "package": "rain.nodes.pandas.transform_nodes.PandasGroupBy",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "key",
          "type": "str",
          "is_mandatory": false,
          "default_value": null,
          "description": "Groupby key, which selects the grouping column of the target."
        },
        {
          "name": "freq",
          "type": "str",
          "is_mandatory": false,
          "default_value": null,
          "description": "This will groupby the specified frequency if the target selection (via key) is a datetime-like object. For full specification of available frequencies, please see `here  /pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`_."
        },
        {
          "name": "axis",
          "type": "int",
          "is_mandatory": false,
          "default_value": 0,
          "description": "Number of the axis."
        },
        {
          "name": "sort",
          "type": "bool",
          "is_mandatory": false,
          "default_value": false,
          "description": "Whether to sort the resulting labels."
        },
        {
          "name": "dropna",
          "type": "bool",
          "is_mandatory": false,
          "default_value": true,
          "description": "If True, and if group keys contain NA values, NA values together with row/column will be dropped. If False, NA values will also be treated as the key in groups."
        },
        {
          "name": "aggregates",
          "type": "str or List[str]",
          "is_mandatory": false,
          "default_value": null,
          "description": "The function used to aggregate the different columns during the GroupBy. It can be either a string, meaning that it will try to apply the chosen aggregation function to all the columns, or a list of strings, one for each column, meaning that it will try to assign a chosen type to each column in order."
        }
      ],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Transformer"
      },
      "description": "PandasGroupBy manages filtering of rows that have been previously selected."
    },
    {
      "clazz": "PandasPivot",
      "package": "rain.nodes.pandas.transform_nodes.PandasPivot",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "rows",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "Name of the column whose values will be the rows of the pivot."
        },
        {
          "name": "columns",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "Name of the column whose values will be the columns of the pivot."
        },
        {
          "name": "values",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "Name of the column whose values will be the values of the pivot."
        },
        {
          "name": "aggfunc",
          "type": "str",
          "is_mandatory": false,
          "default_value": "mean",
          "description": "Function to use for the aggregation."
        },
        {
          "name": "fill_value",
          "type": "int",
          "is_mandatory": false,
          "default_value": 0,
          "description": "Value to replace missing values with."
        },
        {
          "name": "dropna",
          "type": "bool",
          "is_mandatory": false,
          "default_value": true,
          "description": "Do not include columns whose entries are all NaN."
        },
        {
          "name": "sort",
          "type": "bool",
          "is_mandatory": false,
          "default_value": true,
          "description": "Specifies if the result should be sorted."
        }
      ],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Transformer"
      },
      "description": "Transforms a DataFrame into a Pivot table from the given rows, columns and values."
    },
    {
      "clazz": "PandasRenameColumn",
      "package": "rain.nodes.pandas.transform_nodes.PandasRenameColumn",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "columns",
          "type": "list[str]",
          "is_mandatory": true,
          "default_value": null,
          "description": "Column names to assign to the DataFrame. The order is relevant."
        }
      ],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Transformer"
      },
      "description": "Sets column names for a pandas DataFrame."
    },
    {
      "clazz": "PandasReplaceColumn",
      "package": "rain.nodes.pandas.transform_nodes.PandasReplaceColumn",
      "input": {
        "column": "Series"
      },
      "output": {
        "column": "Series"
      },
      "parameter": [
        {
          "name": "first_value",
          "type": "Any",
          "is_mandatory": true,
          "default_value": null,
          "description": "Value used when the condition is True."
        },
        {
          "name": "second_value",
          "type": "Any",
          "is_mandatory": true,
          "default_value": null,
          "description": "Value used when the condition is False."
        }
      ],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Transformer"
      },
      "description": "Node used to replace the boolean values of a Pandas Series with other values given by the user."
    },
    {
      "clazz": "PandasSelectRows",
      "package": "rain.nodes.pandas.transform_nodes.PandasSelectRows",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "selection": "Series",
        "dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "select_nan",
          "type": "bool",
          "is_mandatory": false,
          "default_value": false,
          "description": "Whether to select rows with at least one NaN value."
        },
        {
          "name": "conditions",
          "type": "List[str]",
          "is_mandatory": false,
          "default_value": null,
          "description": "List of conditions to select rows."
        }
      ],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Transformer"
      },
      "description": "PandasSelectRows manages selection of rows, which can later be filtered or deleted."
    },
    {
      "clazz": "PandasSequence",
      "package": "rain.nodes.pandas.transform_nodes.PandasSequence",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "stages",
          "type": "list of PandasTransformer",
          "is_mandatory": true,
          "default_value": null,
          "description": "ordered in an execution sequence. They must all be PandasNodes, hence have a 'dataset' variable used for input and output."
        }
      ],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Transformer"
      },
      "description": "PandasSequence wraps a list of nodes that must be executed in sequence into a single node. Intermediate values are passed along the chain using the 'dataset' variable, hence only PandasNodes can be used within a sequence."
    },
    {
      "clazz": "SplitFeaturesAndLabels",
      "package": "rain.nodes.pandas.transform_nodes.SplitFeaturesAndLabels",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "dataset": "DataFrame",
        "labels": "Series"
      },
      "parameter": [
        {
          "name": "target",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "The name of the column containing the labels."
        }
      ],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Transformer"
      },
      "description": "Node used to split a Dataframe into Features and Labels."
    },
    {
      "clazz": "ZScorePredictor",
      "package": "rain.nodes.pandas.zscore.ZScorePredictor",
      "input": {
        "dataset": "DataFrame",
        "model": "pickle"
      },
      "output": {
        "predictions": "DataFrame"
      },
      "parameter": [
        {
          "name": "columns",
          "type": "List[str]",
          "is_mandatory": false,
          "default_value": [],
          "description": "Column names to apply ZScore to. Empty to use all columns."
        },
        {
          "name": "threshold",
          "type": "float",
          "is_mandatory": false,
          "default_value": 1.3,
          "description": "The threshold of the ZScore to distinguish anomalies."
        }
      ],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Predictor"
      },
      "description": "Node that returns the predictions performed with a ZScore model on the columns of a dataset."
    },
    {
      "clazz": "ZScoreTrainer",
      "package": "rain.nodes.pandas.zscore.ZScoreTrainer",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "model": "pickle"
      },
      "parameter": [
        {
          "name": "columns",
          "type": "List[str]",
          "is_mandatory": false,
          "default_value": [],
          "description": "Column names to apply ZScore to. Empty to use all columns."
        }
      ],
      "methods": null,
      "tags": {
        "library": "Pandas",
        "type": "Trainer"
      },
      "description": "Node that returns the model trained with the ZScore algorithm by analyzing the columns of the dataset."
    },
    {
      "clazz": "PySadPredictor",
      "package": "rain.nodes.pysad.node_structure.PySadPredictor",
      "input": {
        "dataset": "DataFrame",
        "model": "pickle"
      },
      "output": {
        "predictions": "DataFrame"
      },
      "parameter": [],
      "methods": null,
      "tags": {
        "library": "PySad",
        "type": "Predictor"
      },
      "description": "Class representing a PySad predictor, use the given model and dataset to obtain the predictions."
    },
    {
      "clazz": "HalfSpaceTree",
      "package": "rain.nodes.pysad.trainer.HalfSpaceTree",
      "input": {
        "dataset": "DataFrame",
        "labels": "Series"
      },
      "output": {
        "model": "pickle",
        "auroc": "float"
      },
      "parameter": [
        {
          "name": "data",
          "type": null,
          "is_mandatory": true,
          "default_value": null,
          "description": null
        },
        {
          "name": "window_size",
          "type": "int",
          "is_mandatory": false,
          "default_value": 100,
          "description": "The size of the window."
        },
        {
          "name": "num_trees",
          "type": "int",
          "is_mandatory": false,
          "default_value": 25,
          "description": "The number of trees."
        },
        {
          "name": "initial_window_x",
          "type": "np.ndarray",
          "is_mandatory": false,
          "default_value": null,
          "description": "The initial window to fit for initial calibration period. If not None, we simply apply fit to these instances."
        },
        {
          "name": "max_depth",
          "type": "int",
          "is_mandatory": false,
          "default_value": 15,
          "description": "The maximum depth of the trees."
        }
      ],
      "methods": null,
      "tags": {
        "library": "PySad",
        "type": "Trainer"
      },
      "description": "Node that trains a model using the HalfSpaceTree algorithm."
    },
    {
      "clazz": "IForestASD",
      "package": "rain.nodes.pysad.trainer.IForestASD",
      "input": {
        "dataset": "DataFrame",
        "labels": "Series"
      },
      "output": {
        "model": "pickle",
        "auroc": "float"
      },
      "parameter": [
        {
          "name": "window_size",
          "type": "int",
          "is_mandatory": false,
          "default_value": 2048,
          "description": "The size of the reference window and its sliding."
        }
      ],
      "methods": null,
      "tags": {
        "library": "PySad",
        "type": "Trainer"
      },
      "description": "Node that trains a model using the IForestASD algorithm."
    },
    {
      "clazz": "XStream",
      "package": "rain.nodes.pysad.trainer.XStream",
      "input": {
        "dataset": "DataFrame",
        "labels": "Series"
      },
      "output": {
        "model": "pickle",
        "auroc": "float"
      },
      "parameter": [
        {
          "name": "window_size",
          "type": "int",
          "is_mandatory": false,
          "default_value": 25,
          "description": "The size (and the sliding length) of the reference window."
        },
        {
          "name": "num_components",
          "type": "int",
          "is_mandatory": false,
          "default_value": 100,
          "description": "The number of components for streamhash projection."
        },
        {
          "name": "n_chains",
          "type": "int",
          "is_mandatory": false,
          "default_value": 100,
          "description": "The number of half-space chains."
        },
        {
          "name": "depth",
          "type": "int",
          "is_mandatory": false,
          "default_value": 25,
          "description": "The maximum depth for the chains."
        }
      ],
      "methods": null,
      "tags": {
        "library": "PySad",
        "type": "Trainer"
      },
      "description": "Node that trains a model using the xStream algorithm."
    },
    {
      "clazz": "ConformalProbabilityCalibrator",
      "package": "rain.nodes.pysad.transformer.ConformalProbabilityCalibrator",
      "input": {
        "scores": "DataFrame"
      },
      "output": {
        "scores": "DataFrame"
      },
      "parameter": [
        {
          "name": "windowed",
          "type": "bool",
          "is_mandatory": false,
          "default_value": true,
          "description": "Whether the probability calibrator is windowed so that forget scores that are older than `window_size`."
        },
        {
          "name": "window_size",
          "type": "int",
          "is_mandatory": false,
          "default_value": 300,
          "description": "The size of window for running average and std."
        }
      ],
      "methods": null,
      "tags": {
        "library": "PySad",
        "type": "Transformer"
      },
      "description": "This class provides an interface to convert the scores into probabilities through conformal prediction."
    },
    {
      "clazz": "GaussianTailProbabilityCalibrator",
      "package": "rain.nodes.pysad.transformer.GaussianTailProbabilityCalibrator",
      "input": {
        "scores": "DataFrame"
      },
      "output": {
        "scores": "DataFrame"
      },
      "parameter": [
        {
          "name": "running_statistics",
          "type": "bool",
          "is_mandatory": false,
          "default_value": true,
          "description": "Whether to calculate the mean and variance through running window."
        },
        {
          "name": "window_size",
          "type": "int",
          "is_mandatory": false,
          "default_value": 300,
          "description": "The size of window for running average and std. Ignored if `running_statistics` parameter is False."
        }
      ],
      "methods": null,
      "tags": {
        "library": "PySad",
        "type": "Transformer"
      },
      "description": "This class provides an interface to convert the scores into probabilities via Q-function, i.e., the tail  function of Gaussian distribution."
    },
    {
      "clazz": "InstanceUnitNormScaler",
      "package": "rain.nodes.pysad.transformer.InstanceUnitNormScaler",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "pow",
          "type": "float",
          "is_mandatory": false,
          "default_value": 2,
          "description": "The power, for which the norm is calculated. pow=2 is equivalent to the euclidean distance."
        }
      ],
      "methods": null,
      "tags": {
        "library": "PySad",
        "type": "Transformer"
      },
      "description": "A scaler that makes the instance feature vector's norm equal to 1, i.e., the unit vector."
    },
    {
      "clazz": "SimpleKMeans",
      "package": "rain.nodes.sklearn.cluster.SimpleKMeans",
      "input": {
        "fitted_model": "BaseEstimator",
        "dataset": "DataFrame",
        "score_targets": "DataFrame"
      },
      "output": {
        "fitted_model": "BaseEstimator",
        "predictions": "DataFrame",
        "score_value": "float",
        "transformed_dataset": "DataFrame",
        "labels": "DataFrame"
      },
      "parameter": [
        {
          "name": "execute",
          "type": "[fit, predict, score, transform]",
          "is_mandatory": true,
          "default_value": null,
          "description": "List of strings to specify the methods to execute. The allowed strings are those from the _method attribute."
        },
        {
          "name": "n_clusters",
          "type": "int",
          "is_mandatory": false,
          "default_value": 8,
          "description": "The number of clusters to form as well as the number of centroids to generate."
        }
      ],
      "methods": [
        "fit",
        "predict",
        "score",
        "transform"
      ],
      "tags": {
        "library": "Scikit-Learn",
        "type": "Clusterer"
      },
      "description": "A clusterer for the sklearn KMeans that uses the 'sklearn.cluster.KMeans'."
    },
    {
      "clazz": "SklearnPCA",
      "package": "rain.nodes.sklearn.decomposition.SklearnPCA",
      "input": {
        "fitted_model": "BaseEstimator",
        "dataset": "DataFrame",
        "score_targets": "DataFrame"
      },
      "output": {
        "fitted_model": "BaseEstimator",
        "score_value": "float",
        "transformed_dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "execute",
          "type": "[fit, score, transform]",
          "is_mandatory": true,
          "default_value": null,
          "description": "List of strings to specify the methods to execute. The allowed strings are those from the _method attribute."
        },
        {
          "name": "n_components",
          "type": "int",
          "is_mandatory": false,
          "default_value": null,
          "description": "Number of components to keep."
        },
        {
          "name": "whiten",
          "type": "bool",
          "is_mandatory": false,
          "default_value": false,
          "description": "When True (False by default) the components_ vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances."
        },
        {
          "name": "svd_solver",
          "type": "{auto, full, arpack, randomized}",
          "is_mandatory": false,
          "default_value": "auto",
          "description": "Svd solver."
        },
        {
          "name": "tol",
          "type": "float",
          "is_mandatory": false,
          "default_value": 0.0,
          "description": "Tolerance for singular values computed by svd_solver == 'arpack'. Must be positive."
        },
        {
          "name": "iterated_power",
          "type": "int",
          "is_mandatory": false,
          "default_value": "auto",
          "description": "Number of iterations for the power method computed by svd_solver == 'randomized'. Must be positive."
        },
        {
          "name": "random_state",
          "type": "int",
          "is_mandatory": false,
          "default_value": null,
          "description": "Used when the 'arpack' or 'randomized' solvers are used. Pass an int for reproducible results across multiple function calls."
        }
      ],
      "methods": [
        "fit",
        "score",
        "transform"
      ],
      "tags": {
        "library": "Scikit-Learn",
        "type": "Estimator"
      },
      "description": "Node representation of a sklearn PCA estimator that uses the 'sklearn.decomposition.PCA'."
    },
    {
      "clazz": "DaviesBouldinScore",
      "package": "rain.nodes.sklearn.functions.DaviesBouldinScore",
      "input": {
        "samples_dataset": "DataFrame",
        "labels": "DataFrame"
      },
      "output": {
        "score": "float"
      },
      "parameter": [],
      "methods": [],
      "tags": {
        "library": "Scikit-Learn",
        "type": "Metrics"
      },
      "description": "Computes the Davies-Bouldin score using the 'sklearn.metrics.davies_bouldin_score'. The score is defined as the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of within-cluster distances to between-cluster distances. Thus, clusters which are farther apart and less dispersed will result in a better score. The minimum score is zero, with lower values indicating better clustering."
    },
    {
      "clazz": "TrainTestDatasetSplit",
      "package": "rain.nodes.sklearn.functions.TrainTestDatasetSplit",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "train_dataset": "DataFrame",
        "test_dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "test_size",
          "type": "float",
          "is_mandatory": false,
          "default_value": null,
          "description": "The size as percentage of the test dataset (e.g. 0.3 is 30%)."
        },
        {
          "name": "train_size",
          "type": "float",
          "is_mandatory": false,
          "default_value": null,
          "description": "The size as percentage of the train dataset (e.g. 0.7 is 70%)"
        },
        {
          "name": "random_state",
          "type": "int",
          "is_mandatory": false,
          "default_value": null,
          "description": "Seed for the random generation."
        },
        {
          "name": "shuffle",
          "type": "bool",
          "is_mandatory": false,
          "default_value": true,
          "description": "Whether to shuffle the dataset before the splitting."
        }
      ],
      "methods": [],
      "tags": {
        "library": "Scikit-Learn",
        "type": "Transformer"
      },
      "description": "Node that uses the 'sklearn.model_selection.train_test_split' to split a dataset in two parts."
    },
    {
      "clazz": "TrainTestSampleTargetSplit",
      "package": "rain.nodes.sklearn.functions.TrainTestSampleTargetSplit",
      "input": {
        "sample_dataset": "DataFrame",
        "target_dataset": "DataFrame"
      },
      "output": {
        "sample_train_dataset": "DataFrame",
        "sample_test_dataset": "DataFrame",
        "target_train_dataset": "DataFrame",
        "target_test_dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "test_size",
          "type": "float",
          "is_mandatory": false,
          "default_value": null,
          "description": "The size as percentage of the test dataset (e.g. 0.3 is 30%)."
        },
        {
          "name": "train_size",
          "type": "float",
          "is_mandatory": false,
          "default_value": null,
          "description": "The size as percentage of the train dataset (e.g. 0.7 is 70%)"
        },
        {
          "name": "random_state",
          "type": "int",
          "is_mandatory": false,
          "default_value": null,
          "description": "Seed for the random generation."
        },
        {
          "name": "shuffle",
          "type": "bool",
          "is_mandatory": false,
          "default_value": true,
          "description": "Whether to shuffle the dataset before the splitting."
        }
      ],
      "methods": [],
      "tags": {
        "library": "Scikit-Learn",
        "type": "Transformer"
      },
      "description": "Node that uses the 'sklearn.model_selection.train_test_split' to split two datasets in four parts. It is useful for classification where you have to split equally the sample and the target datasets."
    },
    {
      "clazz": "IrisDatasetLoader",
      "package": "rain.nodes.sklearn.loaders.IrisDatasetLoader",
      "input": null,
      "output": {
        "dataset": "DataFrame",
        "target": "DataFrame"
      },
      "parameter": [
        {
          "name": "separate_target",
          "type": "bool",
          "is_mandatory": false,
          "default_value": false,
          "description": "Whether to get the target labels in the separated output 'target'."
        }
      ],
      "methods": null,
      "tags": {
        "library": "Scikit-Learn",
        "type": "Input"
      },
      "description": "Loads the iris dataset as a pandas DataFrame using the 'sklearn.datasets.load_iris'."
    },
    {
      "clazz": "SklearnLinearSVC",
      "package": "rain.nodes.sklearn.svm.SklearnLinearSVC",
      "input": {
        "fitted_model": "BaseEstimator",
        "dataset": "DataFrame",
        "score_targets": "DataFrame",
        "fit_targets": "DataFrame"
      },
      "output": {
        "fitted_model": "BaseEstimator",
        "predictions": "DataFrame",
        "score_value": "float"
      },
      "parameter": [
        {
          "name": "execute",
          "type": "[fit, predict, score]",
          "is_mandatory": true,
          "default_value": null,
          "description": "List of strings to specify the methods to execute. The allowed strings are those from the _method attribute."
        },
        {
          "name": "penalty",
          "type": "str",
          "is_mandatory": false,
          "default_value": "l2",
          "description": "Penalty."
        },
        {
          "name": "loss",
          "type": "str",
          "is_mandatory": false,
          "default_value": "squared_hinge",
          "description": "Loss."
        },
        {
          "name": "dual",
          "type": "bool",
          "is_mandatory": false,
          "default_value": true,
          "description": "Dual."
        },
        {
          "name": "tol",
          "type": "float",
          "is_mandatory": false,
          "default_value": 0.0001,
          "description": "Tol."
        },
        {
          "name": "C",
          "type": "float",
          "is_mandatory": false,
          "default_value": 1.0,
          "description": "C."
        },
        {
          "name": "multi_class",
          "type": "str",
          "is_mandatory": false,
          "default_value": "ovr",
          "description": "Multi_class."
        },
        {
          "name": "fit_intercept",
          "type": "bool",
          "is_mandatory": false,
          "default_value": true,
          "description": "Fit_intercept."
        },
        {
          "name": "intercept_scaling",
          "type": "int",
          "is_mandatory": false,
          "default_value": 1,
          "description": "Intercept_scaling."
        },
        {
          "name": "class_weight",
          "type": "float",
          "is_mandatory": false,
          "default_value": null,
          "description": "Class_weight."
        },
        {
          "name": "verbose",
          "type": "int",
          "is_mandatory": false,
          "default_value": 0,
          "description": "Verbose."
        },
        {
          "name": "random_state",
          "type": "str",
          "is_mandatory": false,
          "default_value": null,
          "description": "Random_state."
        },
        {
          "name": "max_iter",
          "type": "int",
          "is_mandatory": false,
          "default_value": 1000,
          "description": "Max_iter."
        }
      ],
      "methods": [
        "fit",
        "predict",
        "score"
      ],
      "tags": {
        "library": "Scikit-Learn",
        "type": "Classifier"
      },
      "description": "Node that uses the 'sklearn.svm.LinearSVC' classifier."
    },
    {
      "clazz": "SparkColumnSelector",
      "package": "rain.nodes.spark.data_wrangling.SparkColumnSelector",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "column_list",
          "type": "List[str]",
          "is_mandatory": true,
          "default_value": null,
          "description": "List of columns to select from the dataset"
        },
        {
          "name": "filter_list",
          "type": "List[str]",
          "is_mandatory": false,
          "default_value": [],
          "description": "List of conditions used to filter the rows of the dataset"
        }
      ],
      "methods": null,
      "tags": {
        "library": "PySpark",
        "type": "Transformer"
      },
      "description": "SparkColumnSelector manages filtering of rows, columns and values for a Spark DataFrame."
    },
    {
      "clazz": "SparkSplitDataset",
      "package": "rain.nodes.spark.data_wrangling.SparkSplitDataset",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "dataset": "DataFrame",
        "train_dataset": "DataFrame",
        "test_dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "train",
          "type": "float",
          "is_mandatory": true,
          "default_value": null,
          "description": "Percentage of the dataset to split into a train dataset."
        },
        {
          "name": "test",
          "type": "float",
          "is_mandatory": true,
          "default_value": null,
          "description": "Percentage of the dataset to split into a test dataset."
        }
      ],
      "methods": null,
      "tags": {
        "library": "PySpark",
        "type": "Transformer"
      },
      "description": "Splits a Spark DataFrame in two DataFrames, train and test."
    },
    {
      "clazz": "SparkPipelineNode",
      "package": "rain.nodes.spark.pipeline.spark_pipeline.SparkPipelineNode",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "model": "PipelineModel"
      },
      "parameter": [
        {
          "name": "stages",
          "type": null,
          "is_mandatory": true,
          "default_value": null,
          "description": null
        }
      ],
      "methods": null,
      "tags": {
        "library": "PySpark",
        "type": "Estimator"
      },
      "description": "Represent a Spark Pipeline consisting of SparkNode (stages). It should contain some Spark Transformer and a final Spark Estimator that return the trained model."
    },
    {
      "clazz": "HashingTF",
      "package": "rain.nodes.spark.pipeline.stages.HashingTF",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "in_col",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "The name of the input column."
        },
        {
          "name": "out_col",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "The name of the output column."
        }
      ],
      "methods": null,
      "tags": {
        "library": "PySpark",
        "type": "Transformer"
      },
      "description": "Represent a Spark HashingTF that maps a sequence of terms to their term frequencies using the hashing trick."
    },
    {
      "clazz": "LogisticRegression",
      "package": "rain.nodes.spark.pipeline.stages.LogisticRegression",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "model": "PipelineModel"
      },
      "parameter": [
        {
          "name": "max_iter",
          "type": "int",
          "is_mandatory": true,
          "default_value": null,
          "description": "Max number of iterations."
        },
        {
          "name": "reg_param",
          "type": "float",
          "is_mandatory": true,
          "default_value": null,
          "description": "Regularization parameter."
        }
      ],
      "methods": null,
      "tags": {
        "library": "PySpark",
        "type": "Estimator"
      },
      "description": "Represent a SparkNode that supports fitting traditional logistic regression model."
    },
    {
      "clazz": "Tokenizer",
      "package": "rain.nodes.spark.pipeline.stages.Tokenizer",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "in_col",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "The name of the input column."
        },
        {
          "name": "out_col",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "The name of the output column."
        }
      ],
      "methods": null,
      "tags": {
        "library": "PySpark",
        "type": "Transformer"
      },
      "description": "Represent a Spark Tokenizer used to split text in individual term."
    },
    {
      "clazz": "SparkCSVLoader",
      "package": "rain.nodes.spark.spark_input.SparkCSVLoader",
      "input": null,
      "output": {
        "dataset": "DataFrame"
      },
      "parameter": [
        {
          "name": "path",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "Path of the csv file."
        },
        {
          "name": "header",
          "type": "bool",
          "is_mandatory": false,
          "default_value": false,
          "description": "Uses the first line as names of columns."
        },
        {
          "name": "schema",
          "type": "bool",
          "is_mandatory": false,
          "default_value": false,
          "description": "Infers the input schema automatically from data. It requires one extra pass over the data."
        }
      ],
      "methods": null,
      "tags": {
        "library": "PySpark",
        "type": "Input"
      },
      "description": "Loads a CSV file as a Spark DataFrame."
    },
    {
      "clazz": "SparkModelLoader",
      "package": "rain.nodes.spark.spark_input.SparkModelLoader",
      "input": null,
      "output": {
        "model": "PipelineModel"
      },
      "parameter": [
        {
          "name": "path",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "Path of the csv file."
        }
      ],
      "methods": null,
      "tags": {
        "library": "PySpark",
        "type": "Input"
      },
      "description": "Loads a file as a Spark Model."
    },
    {
      "clazz": "SparkSaveDataset",
      "package": "rain.nodes.spark.spark_output.SparkSaveDataset",
      "input": {
        "dataset": "DataFrame"
      },
      "output": null,
      "parameter": [
        {
          "name": "path",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "String representing the path where to save the dataset"
        },
        {
          "name": "index",
          "type": "bool",
          "is_mandatory": false,
          "default_value": true,
          "description": "String representing the path where to save the dataset"
        }
      ],
      "methods": null,
      "tags": {
        "library": "PySpark",
        "type": "Output"
      },
      "description": "Save a Spark Dataframe in a .csv format"
    },
    {
      "clazz": "SparkSaveModel",
      "package": "rain.nodes.spark.spark_output.SparkSaveModel",
      "input": {
        "model": "PipelineModel"
      },
      "output": null,
      "parameter": [
        {
          "name": "path",
          "type": "str",
          "is_mandatory": true,
          "default_value": null,
          "description": "String representing the path where to save the model."
        }
      ],
      "methods": null,
      "tags": {
        "library": "PySpark",
        "type": "Output"
      },
      "description": "Save a trained PipelineModel"
    },
    {
      "clazz": "TPOTClassificationPredictor",
      "package": "rain.nodes.tpot.classification.TPOTClassificationPredictor",
      "input": {
        "dataset": "DataFrame",
        "model": "pickle"
      },
      "output": {
        "predictions": "DataFrame"
      },
      "parameter": [],
      "methods": null,
      "tags": {
        "library": "TPOT",
        "type": "Predictor"
      },
      "description": "Node that returns the predictions performed with a TPOT Classification model on the columns of a dataset without the target feature column."
    },
    {
      "clazz": "TPOTClassificationTrainer",
      "package": "rain.nodes.tpot.classification.TPOTClassificationTrainer",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "code": "str",
        "model": "pickle"
      },
      "parameter": [
        {
          "name": "target_feature",
          "type": "str",
          "is_mandatory": false,
          "default_value": null,
          "description": "Name of the target feature."
        },
        {
          "name": "export_script",
          "type": "bool",
          "is_mandatory": false,
          "default_value": false,
          "description": "Whether to export the resulting Python script."
        },
        {
          "name": "generations",
          "type": "int",
          "is_mandatory": false,
          "default_value": 100,
          "description": "Number of iterations to the run pipeline optimization process. It must be a positive number. If not set, the parameter max_time_mins must be defined as the runtime limit. Generally, TPOT will work better when you give it more generations (and therefore time) to optimize the pipeline. TPOT will evaluate POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total."
        },
        {
          "name": "population_size",
          "type": "int",
          "is_mandatory": false,
          "default_value": 100,
          "description": "Number of individuals to retain in the GP population every generation. Generally, TPOT will work better when you give it more individuals (and therefore time) to optimize the pipeline. TPOT will evaluate POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total."
        },
        {
          "name": "offspring_size",
          "type": "int",
          "is_mandatory": false,
          "default_value": null,
          "description": "Number of offspring to produce in each GP generation. By default, offspring_size = population_size."
        },
        {
          "name": "mutation_rate",
          "type": "float",
          "is_mandatory": false,
          "default_value": 0.9,
          "description": "Mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This parameter tells the GP algorithm how many pipelines to apply random changes to every generation. We recommend using the default parameter unless you understand how the mutation rate affects GP algorithms."
        },
        {
          "name": "crossover_rate",
          "type": "float",
          "is_mandatory": false,
          "default_value": 0.1,
          "description": "Crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This parameter tells the genetic programming algorithm how many pipelines to \"breed\" every generation. We recommend using the default parameter unless you understand how the mutation rate affects GP algorithms."
        },
        {
          "name": "scoring",
          "type": "str",
          "is_mandatory": false,
          "default_value": "accuracy",
          "description": "Function used to evaluate the quality of a given pipeline for the problem. By default, accuracy is used for classification problems. Offers the same options as sklearn.model_selection.cross_val_score as well as a built-in score 'balanced_accuracy'. Classification metrics: ['accuracy', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']"
        },
        {
          "name": "cv",
          "type": "int",
          "is_mandatory": false,
          "default_value": 5,
          "description": "The number of folds to evaluate each pipeline over in k-fold cross-validation during the TPOT optimization process."
        },
        {
          "name": "subsample",
          "type": "float",
          "is_mandatory": false,
          "default_value": 1.0,
          "description": "Subsample ratio of the training instance. Setting it to 0.5 means that TPOT randomly collects half of training samples for pipeline optimization process."
        },
        {
          "name": "n_jobs",
          "type": "int",
          "is_mandatory": false,
          "default_value": 1,
          "description": "Number of CPUs for evaluating pipelines in parallel during the TPOT optimization process. Assigning this to -1 will use as many cores as available on the computer. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used."
        },
        {
          "name": "max_time_mins",
          "type": "int",
          "is_mandatory": false,
          "default_value": null,
          "description": "How many minutes TPOT has to optimize the pipeline. If not None, this setting will allow TPOT to run until max_time_mins minutes elapsed and then stop. TPOT will stop earlier if generationsis set and all generations are already evaluated."
        },
        {
          "name": "max_eval_time_mins",
          "type": "float",
          "is_mandatory": false,
          "default_value": 5,
          "description": "How many minutes TPOT has to optimize a single pipeline. Setting this parameter to higher values will allow TPOT to explore more complex pipelines, but will also allow TPOT to run longer."
        },
        {
          "name": "random_state",
          "type": "int",
          "is_mandatory": false,
          "default_value": null,
          "description": "Random number generator seed for TPOT. Use this parameter to make sure that TPOT will give you the same results each time you run it against the same data set with that seed."
        },
        {
          "name": "config_dict",
          "type": "{TPOT light, TPOT MDR, TPOT sparse, TPOT NN}",
          "is_mandatory": false,
          "default_value": null,
          "description": "String 'TPOT light':     TPOT uses a light version of operator configuration dictionary instead of     the default one. String 'TPOT MDR':     TPOT uses a list of TPOT-MDR operator configuration dictionary instead of     the default one. String 'TPOT sparse':     TPOT uses a configuration dictionary with a one-hot-encoder and the     operators normally included in TPOT that also support sparse matrices. String 'TPOT NN':     TPOT uses a configuration dictionary for PyTorch neural network classifiers     included in `tpot.nn`."
        },
        {
          "name": "template",
          "type": "str",
          "is_mandatory": false,
          "default_value": null,
          "description": "Template of predefined pipeline structure. The option is for specifying a desired structure for the machine learning pipeline evaluated in TPOT. So far this option only supports linear pipeline structure. Each step in the pipeline should be a main class of operators (Selector, Transformer, Classifier or Regressor) or a specific operator (e.g. SelectPercentile) defined in TPOT operator configuration. If one step is a main class, TPOT will randomly assign all subclass operators (subclasses of SelectorMixin, TransformerMixin, ClassifierMixin or RegressorMixin in scikit-learn) to that step. Steps in the template are delimited by \"-\", e.g. \"SelectPercentile-Transformer-Classifier\". By default value of template is None, TPOT generates tree-based pipeline randomly."
        },
        {
          "name": "warm_start",
          "type": "bool",
          "is_mandatory": false,
          "default_value": false,
          "description": "Flag indicating whether the TPOT instance will reuse the population from previous calls to fit()."
        },
        {
          "name": "memory",
          "type": "str",
          "is_mandatory": false,
          "default_value": null,
          "description": "If supplied, pipeline will cache each transformer after calling fit. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. String 'auto':     TPOT uses memory caching with a temporary directory and cleans it up upon shutdown. String path of a caching directory     TPOT uses memory caching with the provided directory and TPOT does NOT clean     the caching directory up upon shutdown. If the directory does not exist, TPOT will     create it. None:     TPOT does not use memory caching."
        },
        {
          "name": "use_dask",
          "type": "bool",
          "is_mandatory": false,
          "default_value": false,
          "description": "Whether to use Dask-ML's pipeline optimizations. This avoid re-fitting the same estimator on the same split of data multiple times. It will also provide more detailed diagnostics when using Dask's distributed scheduler."
        },
        {
          "name": "periodic_checkpoint_folder",
          "type": "str",
          "is_mandatory": false,
          "default_value": null,
          "description": "If supplied, a folder in which tpot will periodically save pipelines in pareto front so far while optimizing. Currently once per generation but not more often than once per 30 seconds. Useful in multiple cases:     Sudden death before tpot could save optimized pipeline     Track its progress     Grab pipelines while it's still optimizing"
        },
        {
          "name": "early_stop",
          "type": "int",
          "is_mandatory": false,
          "default_value": null,
          "description": "How many generations TPOT checks whether there is no improvement in optimization process. End optimization process if there is no improvement in the set number of generations."
        },
        {
          "name": "verbosity",
          "type": "int",
          "is_mandatory": false,
          "default_value": 0,
          "description": "How much information TPOT communicates while it's running. 0 = none, 1 = minimal, 2 = high, 3 = all. A setting of 2 or higher will add a progress bar during the optimization procedure."
        },
        {
          "name": "log_file",
          "type": "str",
          "is_mandatory": false,
          "default_value": null,
          "description": "Save progress content to a file."
        }
      ],
      "methods": null,
      "tags": {
        "library": "TPOT",
        "type": "Trainer"
      },
      "description": "Node that returns the classification model trained with the TPOT library."
    },
    {
      "clazz": "TPOTRegressionPredictor",
      "package": "rain.nodes.tpot.regression.TPOTRegressionPredictor",
      "input": {
        "dataset": "DataFrame",
        "model": "pickle"
      },
      "output": {
        "predictions": "DataFrame"
      },
      "parameter": [],
      "methods": null,
      "tags": {
        "library": "TPOT",
        "type": "Predictor"
      },
      "description": "Node that returns the predictions performed with a TPOT Regression model on the columns of a dataset without the target feature column."
    },
    {
      "clazz": "TPOTRegressionTrainer",
      "package": "rain.nodes.tpot.regression.TPOTRegressionTrainer",
      "input": {
        "dataset": "DataFrame"
      },
      "output": {
        "code": "str",
        "model": "pickle"
      },
      "parameter": [
        {
          "name": "target_feature",
          "type": "str",
          "is_mandatory": false,
          "default_value": null,
          "description": "Name of the target feature."
        },
        {
          "name": "export_script",
          "type": "bool",
          "is_mandatory": false,
          "default_value": false,
          "description": "Whether to export the resulting Python script."
        },
        {
          "name": "generations",
          "type": "int",
          "is_mandatory": false,
          "default_value": 100,
          "description": "Number of iterations to the run pipeline optimization process. It must be a positive number. If not set, the parameter max_time_mins must be defined as the runtime limit. Generally, TPOT will work better when you give it more generations (and therefore time) to optimize the pipeline. TPOT will evaluate POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total."
        },
        {
          "name": "population_size",
          "type": "int",
          "is_mandatory": false,
          "default_value": 100,
          "description": "Number of individuals to retain in the GP population every generation. Generally, TPOT will work better when you give it more individuals (and therefore time) to optimize the pipeline. TPOT will evaluate POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total."
        },
        {
          "name": "offspring_size",
          "type": "int",
          "is_mandatory": false,
          "default_value": null,
          "description": "Number of offspring to produce in each GP generation. By default, offspring_size = population_size."
        },
        {
          "name": "mutation_rate",
          "type": "float",
          "is_mandatory": false,
          "default_value": 0.9,
          "description": "Mutation rate for the genetic programming algorithm in the range [0.0, 1.0]. This parameter tells the GP algorithm how many pipelines to apply random changes to every generation. We recommend using the default parameter unless you understand how the mutation rate affects GP algorithms."
        },
        {
          "name": "crossover_rate",
          "type": "float",
          "is_mandatory": false,
          "default_value": 0.1,
          "description": "Crossover rate for the genetic programming algorithm in the range [0.0, 1.0]. This parameter tells the genetic programming algorithm how many pipelines to \"breed\" every generation. We recommend using the default parameter unless you understand how the mutation rate affects GP algorithms."
        },
        {
          "name": "scoring",
          "type": "str",
          "is_mandatory": false,
          "default_value": "neg_mean_squared_error",
          "description": "Function used to evaluate the quality of a given pipeline for the problem. By default, mean squared error (MSE) is used for regression problems. Offers the same options as sklearn.model_selection.cross_val_score as well as a built-in score 'balanced_accuracy'. Regression metrics: ['neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'r2']"
        },
        {
          "name": "cv",
          "type": "int",
          "is_mandatory": false,
          "default_value": 5,
          "description": "The number of folds to evaluate each pipeline over in k-fold cross-validation during the TPOT optimization process."
        },
        {
          "name": "subsample",
          "type": "float",
          "is_mandatory": false,
          "default_value": 1.0,
          "description": "Subsample ratio of the training instance. Setting it to 0.5 means that TPOT randomly collects half of training samples for pipeline optimization process."
        },
        {
          "name": "n_jobs",
          "type": "int",
          "is_mandatory": false,
          "default_value": 1,
          "description": "Number of CPUs for evaluating pipelines in parallel during the TPOT optimization process. Assigning this to -1 will use as many cores as available on the computer. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used."
        },
        {
          "name": "max_time_mins",
          "type": "int",
          "is_mandatory": false,
          "default_value": null,
          "description": "How many minutes TPOT has to optimize the pipeline. If not None, this setting will allow TPOT to run until max_time_mins minutes elapsed and then stop. TPOT will stop earlier if generationsis set and all generations are already evaluated."
        },
        {
          "name": "max_eval_time_mins",
          "type": "float",
          "is_mandatory": false,
          "default_value": 5,
          "description": "How many minutes TPOT has to optimize a single pipeline. Setting this parameter to higher values will allow TPOT to explore more complex pipelines, but will also allow TPOT to run longer."
        },
        {
          "name": "random_state",
          "type": "int",
          "is_mandatory": false,
          "default_value": null,
          "description": "Random number generator seed for TPOT. Use this parameter to make sure that TPOT will give you the same results each time you run it against the same data set with that seed."
        },
        {
          "name": "config_dict",
          "type": "{TPOT light, TPOT MDR, TPOT sparse, TPOT NN}",
          "is_mandatory": false,
          "default_value": null,
          "description": "String 'TPOT light':     TPOT uses a light version of operator configuration dictionary instead of     the default one. String 'TPOT MDR':     TPOT uses a list of TPOT-MDR operator configuration dictionary instead of     the default one. String 'TPOT sparse':     TPOT uses a configuration dictionary with a one-hot-encoder and the     operators normally included in TPOT that also support sparse matrices. String 'TPOT NN':     TPOT uses a configuration dictionary for PyTorch neural network classifiers     included in `tpot.nn`."
        },
        {
          "name": "template",
          "type": "str",
          "is_mandatory": false,
          "default_value": null,
          "description": "Template of predefined pipeline structure. The option is for specifying a desired structure for the machine learning pipeline evaluated in TPOT. So far this option only supports linear pipeline structure. Each step in the pipeline should be a main class of operators (Selector, Transformer, Classifier or Regressor) or a specific operator (e.g. SelectPercentile) defined in TPOT operator configuration. If one step is a main class, TPOT will randomly assign all subclass operators (subclasses of SelectorMixin, TransformerMixin, ClassifierMixin or RegressorMixin in scikit-learn) to that step. Steps in the template are delimited by \"-\", e.g. \"SelectPercentile-Transformer-Classifier\". By default value of template is None, TPOT generates tree-based pipeline randomly."
        },
        {
          "name": "warm_start",
          "type": "bool",
          "is_mandatory": false,
          "default_value": false,
          "description": "Flag indicating whether the TPOT instance will reuse the population from previous calls to fit()."
        },
        {
          "name": "memory",
          "type": "str",
          "is_mandatory": false,
          "default_value": null,
          "description": "If supplied, pipeline will cache each transformer after calling fit. This feature is used to avoid computing the fit transformers within a pipeline if the parameters and input data are identical with another fitted pipeline during optimization process. String 'auto':     TPOT uses memory caching with a temporary directory and cleans it up upon shutdown. String path of a caching directory     TPOT uses memory caching with the provided directory and TPOT does NOT clean     the caching directory up upon shutdown. If the directory does not exist, TPOT will     create it. None:     TPOT does not use memory caching."
        },
        {
          "name": "use_dask",
          "type": "bool",
          "is_mandatory": false,
          "default_value": false,
          "description": "Whether to use Dask-ML's pipeline optimizations. This avoid re-fitting the same estimator on the same split of data multiple times. It will also provide more detailed diagnostics when using Dask's distributed scheduler."
        },
        {
          "name": "periodic_checkpoint_folder",
          "type": "str",
          "is_mandatory": false,
          "default_value": null,
          "description": "If supplied, a folder in which tpot will periodically save pipelines in pareto front so far while optimizing. Currently once per generation but not more often than once per 30 seconds. Useful in multiple cases:     Sudden death before tpot could save optimized pipeline     Track its progress     Grab pipelines while it's still optimizing"
        },
        {
          "name": "early_stop",
          "type": "int",
          "is_mandatory": false,
          "default_value": null,
          "description": "How many generations TPOT checks whether there is no improvement in optimization process. End optimization process if there is no improvement in the set number of generations."
        },
        {
          "name": "verbosity",
          "type": "int",
          "is_mandatory": false,
          "default_value": 0,
          "description": "How much information TPOT communicates while it's running. 0 = none, 1 = minimal, 2 = high, 3 = all. A setting of 2 or higher will add a progress bar during the optimization procedure."
        }
      ],
      "methods": null,
      "tags": {
        "library": "TPOT",
        "type": "Trainer"
      },
      "description": "Node that returns the regression model trained with the TPOT library."
    }
  ],
  "dependencies": [
    "pandas~=1.3.0",
    "scikit-learn~=0.24.2",
    "pyspark~=3.1.2",
    "pymongo~=3.12.0",
    "tpot==0.11.7",
    "pysad==0.1.1"
  ]
}